{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis\n",
    "\n",
    "This notebook focuses on analysing the word clusters for a model. This includes visualizing fit of the clusters, formating them for manual inspection, and visualizing them using Multi Dimensional Scaling (MDS).\n",
    "\n",
    "We first import the libaries we will need throughout the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import graphing utilities\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import useful mathematical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import useful Machine learning libraries\n",
    "import gensim\n",
    "\n",
    "# Import utility files\n",
    "from utils import save_object,load_object, make_clustering_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Setup directories\n",
    "\n",
    "If this is the first time doing this analysis, \n",
    "we first will set up all the directories we need\n",
    "to save and load the models we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "directories = ['cluster-analysis']\n",
    "for dirname in directories:\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model name\n",
    "\n",
    "Before begining the rest of this project, we select a name for our model. This name will be used to save and load the files for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the model we are going to be analyzing\n",
    "model_name = \"example_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure fit\n",
    "\n",
    "Now that we have initialized all we need for our analysis, we can procceed to examine the fit of each clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the fit and test point values\n",
    "fit = load_object('objects/', model_name + \"-words\" + \"-fit\")\n",
    "test_points = load_object('objects/', model_name + \"-words\" + \"-test_points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the fit for each size\n",
    "plt.plot(test_points, fit, 'ro')\n",
    "plt.axis([0, 400, 0, np.ceil(fit[0] + (1/10)*fit[0])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format for inspection\n",
    "\n",
    "After measuring the fit of each clustering, we can decide the number of clusters to use, and further focus on this clustering. To better examine this clustering, we convert the clustering into an readable csv here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the number of clusters to analyze\n",
    "num_clusters = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the models\n",
    "model = gensim.models.Word2Vec.load('models/' + model_name + '.model')\n",
    "kmeans = load_object('clusters/', model_name + \"-words-cluster_model-\" + str(num_clusters))\n",
    "WordsByFeatures = load_object('matricies/', model_name + '-' + 'WordsByFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_list = sorted(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = make_clustering_objects(model, kmeans, vocab_list, WordsByFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort all the words in the words list\n",
    "for cluster in clusters:\n",
    "    cluster[\"word_list\"].sort(key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the number of words to display. The table with contain the top size_words_list words\n",
    "size_words_list = 100\n",
    "table = []\n",
    "for i in range(len(clusters)):\n",
    "    row = []\n",
    "    row.append(\"cluster \" + str(i+1))\n",
    "    row.append(clusters[i][\"total_freq\"])\n",
    "    row.append(clusters[i][\"unique_words\"])\n",
    "    for j in range(size_words_list):\n",
    "        try:\n",
    "            row.append(clusters[i][\"word_list\"][j])\n",
    "        except:\n",
    "            break\n",
    "    table.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('cluster-analysis/' + model_name + \"-\" + str(num_clusters) + '.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    [writer.writerow(r) for r in table]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Clusters Using MDS\n",
    "\n",
    "Produce a visualization of our clusters in a low dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model to the clusters\n",
    "from sklearn.manifold import MDS\n",
    "mds = MDS().fit(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the embeddings\n",
    "embedding = mds.embedding_.tolist()\n",
    "x = list(map(lambda x:x[0], embedding))\n",
    "y = list(map(lambda x:x[1], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words= list(map(lambda x: x[0][0], map(lambda x: x[\"word_list\"], clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the Graph with top words\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(x, y, 'bo')\n",
    "for i in range(len(top_words)):\n",
    "    plt.annotate(top_words[i], (x[i], y[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def helper(indicies,points):\n",
    "    temp = []\n",
    "    for i in indicies:\n",
    "        temp.append(points[i-1])\n",
    "    return temp\n",
    "\n",
    "## All categories are examples, not used in any current model\n",
    "\n",
    "bullying = [59, 16, 47]\n",
    "crime = [31, 73]\n",
    "depressive_feelings = [1, 3, 15, 21, 29, 45, 81, 4, 30]\n",
    "depressive_symptoms = [9, 13, 28] \n",
    "drug_abuse =[22, 41, 75]\n",
    "illness = [35, 87]\n",
    "failure = [68, 89, 90, 14, 19, 26, 52]\n",
    "prior_suicide = [27, 56, 79]\n",
    "psychological =[78, 10, 44, 66, 85]\n",
    "self_harm = [5, 17]\n",
    "self_image = [69, 8, 96]\n",
    "death_around = [76, 93]\n",
    "suicidal_ideation = [36, 38, 57, 58, 97, 6]\n",
    "identified = bullying + crime + depressive_feelings + depressive_symptoms\n",
    "identified = identified + drug_abuse + illness + failure + prior_suicide + psychological\n",
    "identified = identified + self_harm + self_image + death_around + suicidal_ideation\n",
    "other = [x for x in range(1, 101) if x not in identified]\n",
    "all_categories = [bullying, crime, depressive_feelings, depressive_symptoms,\n",
    "                  drug_abuse, illness, failure, prior_suicide, psychological,\n",
    "                  self_harm, self_image, death_around, suicidal_ideation, other]\n",
    "colors = [\"black\" for x in all_categories]\n",
    "\n",
    "\"\"\"\n",
    "colors = [\"#ff66ff\", \"#6666ff\", \"#000099\",\n",
    "          \"#33cccc\", \"#00cc66\", \"#336600\",\n",
    "          \"#ccff33\", \"#cc6600\", \"#ff0000\",\n",
    "          \"#cc0066\", \"#ffccff\", \"#ccffff\", \"#00ff00\", \"#00ffff\"]\n",
    "\"\"\"\n",
    "#colors[0]= \"grey\"  # Bullying\n",
    "#colors[2]= \"red\"   # Depressive Feelings\n",
    "#colors[4]= \"green\" # Drug Abuse\n",
    "#colors[6]= \"blue\"  # Poor performance\n",
    "#colors[3]= \"magenta\" # Depressive symptoms\n",
    "#colors[8]= \"cyan\" # Psychological \n",
    "\n",
    "\n",
    "# Plot the Graph with top words\n",
    "plt.figure(figsize = (10, 5))\n",
    "for i in range(len(all_categories)):\n",
    "    category = all_categories[i]\n",
    "    color = colors[i]\n",
    "    plt.scatter(helper(category, x), helper(category, y), color = color, s = 100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
