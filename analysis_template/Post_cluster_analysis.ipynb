{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Cluster Analysis\n",
    "\n",
    "This notebook focuses on analysing the posts clusters for a model. We will do this by first generating \n",
    "We first import the libaries we will need throughout the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import graphing utilities\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import useful mathematical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import useful Machine learning libraries\n",
    "import gensim\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Import utility files\n",
    "from utils import save_object, load_object, make_post_clusters, make_clustering_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup directories\n",
    "\n",
    "If this is the first time doing this analysis, \n",
    "we first will set up all the directories we need\n",
    "to save and load the models we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "directories = ['post-analysis']\n",
    "for dirname in directories:\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model name\n",
    "\n",
    "Before begining the rest of this project, we select a name for our model. This name will be used to save and load the files for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the model we are going to be analyzing\n",
    "model_name = \"model6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "We now load and process the data we will need for the rest of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = load_object('objects/', model_name + '-df')\n",
    "\n",
    "scores = list(df['score'])\n",
    "num_comments_list = list(df['num_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131652"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Our Saved matricies\n",
    "PostsByWords = load_object('matricies/', model_name + \"-PostsByWords\")\n",
    "WordsByFeatures = load_object('matricies/', model_name + \"-WordsByFeatures\")\n",
    "\n",
    "# Generate the posts by Features matrix through matrix multiplication\n",
    "PostsByFeatures = PostsByWords.dot(WordsByFeatures)\n",
    "PostsByFeatures = np.matrix(PostsByFeatures)\n",
    "len(PostsByFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('models/' + model_name + '.model')\n",
    "\n",
    "vocab_list = sorted(list(model.wv.vocab))\n",
    "\n",
    "# Initialize a word clustering to use\n",
    "num_word_clusters = 100\n",
    "kmeans =  load_object('clusters/', model_name + '-words-cluster_model-' + str(num_word_clusters))\n",
    "\n",
    "clusters = make_clustering_objects(model, kmeans, vocab_list, WordsByFeatures)\n",
    "\n",
    "clusterWords = list(map(lambda x: list(map(lambda y: y[0] , x[\"word_list\"])), clusters))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer(vocabulary = vocab_list, analyzer = (lambda lst:list(map((lambda s: s), lst))), min_df=0)\n",
    "\n",
    "# Make Clusters By Words Matrix\n",
    "ClustersByWords = countvec.fit_transform(clusterWords)\n",
    "\n",
    "# Ensure consistency\n",
    "len(WordsByFeatures) == ClustersByWords.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take the transpose of Clusters\n",
    "WordsByCluster = ClustersByWords.transpose()\n",
    "\n",
    "# Multiply Posts by Words by Words By cluster to get Posts By cluster\n",
    "PostsByClusters = PostsByWords.dot(WordsByCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PostsByClusters = PostsByClusters.todense() * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_min = PostsByFeatures.min(axis = 1)\n",
    "row_max = PostsByFeatures.max(axis = 1)\n",
    "row_diff_normed = (row_max - row_min == 0) + (row_max - row_min)\n",
    "PostsByFeaturesNormed = (PostsByFeatures - row_min) / row_diff_normed\n",
    "\n",
    "row_min = PostsByClusters.min(axis = 1)\n",
    "row_max = PostsByClusters.max(axis = 1)\n",
    "row_diff_normed = (row_max - row_min == 0) + (row_max - row_min)\n",
    "PostsByClustersNormed = (PostsByClusters - row_min) / row_diff_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array(PostsByClusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts_df = pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows, columns = posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "correlation_table =[]\n",
    "for i in range(columns): # rows are the number of rows in the matrix. \n",
    "    correlation_row = []\n",
    "    for j in range(columns):\n",
    "        r = scipy.stats.pearsonr(a[:,i], a[:,j])\n",
    "        correlation_row.append(r[0])\n",
    "    correlation_table.append(correlation_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10942775947734305, 0.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.pearsonr(a[:,19], a[:,18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131652"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a[:,19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print Correlation table\n",
    "import csv\n",
    "header = [\"Cluster \"+ str(i) for i in range(1,columns+1)]\n",
    "with open('cluster-analysis/' + \"correlation-\"+model_name + \"-\" + str(num_word_clusters) + '.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"\"]+header)\n",
    "    for i in range(len(correlation_table)):\n",
    "        writer.writerow([header[i]]+correlation_table[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Post Clusters\n",
    "\n",
    "We now will generate post clusters, and then save them in a format conducive to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_posts_clusters =10\n",
    "matricies = [PostsByFeatures, PostsByClusters, PostsByFeaturesNormed, PostsByClustersNormed]\n",
    "names     = [\"byFeatures\", \"byClusters\", \"byFeatures-Normed\", \"byClusters-Normed\"]\n",
    "mat_names = list(zip(matricies, names))\n",
    "post_dfs  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for mat,name in mat_names:\n",
    "    #initialize kmeans model\n",
    "    kmeans = KMeans(n_clusters = num_posts_clusters, random_state = 42).fit(mat)\n",
    "    # Save the clusters directory\n",
    "    save_object(kmeans, 'clusters/', model_name + \"-posts-\" + name + \"-\" + str(num_posts_clusters))\n",
    "    del kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup the header for the CSV files\n",
    "header = ['total_posts','score_mean','score_median','score_range','comments_mean','comments_median','comments_range']\n",
    "# Loop over all matricies\n",
    "for mat,name in mat_names:\n",
    "    # Load Clusters\n",
    "    kmeans= load_object('clusters/',model_name+\"-posts-\"+name+\"-\"+str(num_posts_clusters))\n",
    "    # Generate Post_clusters\n",
    "    post_clusters = make_post_clusters(kmeans,mat,scores,num_comments_list)\n",
    "    temp_header =header+list(map(lambda x:\"element \"+str(x),range(1,mat.shape[1]+1)))\n",
    "    temp_table = list(map(lambda x: list(map(lambda y: x[1][y],header))+\n",
    "                          list(map(lambda z: z[0],post_clusters[x[0]]['center'])),enumerate(post_clusters)))\n",
    "    #post_dfs.append(pd.DataFrame.from_records(temp_table,columns =temp_header))\n",
    "\n",
    "    import csv\n",
    "    with open('post-analysis/' + model_name + '-' + str(num_posts_clusters) + '-' + name + '.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(temp_header)\n",
    "        [writer.writerow(r) for r in temp_table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans= load_object('clusters/',model_name+\"-posts-\"+\"byClusters\"+\"-\"+str(num_posts_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(kmeans.cluster_centers_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
