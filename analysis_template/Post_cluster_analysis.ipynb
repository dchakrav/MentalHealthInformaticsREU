{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Cluster Analysis\n",
    "\n",
    "This notebook focuses on analysing the posts clusters for a model. We will do this by first generating \n",
    "We first import the libaries we will need throughout the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import graphing utilities\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import useful mathematical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import useful Machine learning libraries\n",
    "import gensim\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Import utility files\n",
    "from utils import save_object,load_object, make_post_clusters, make_clustering_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup directories\n",
    "\n",
    "If this is the first time doing this analysis, \n",
    "we first will set up all the directories we need\n",
    "to save and load the models we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "directories = ['post-analysis']\n",
    "for dirname in directories:\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model name\n",
    "\n",
    "Before begining the rest of this project, we select a name for our model. This name will be used to save and load the files for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the model we are going to be analyzing\n",
    "model_name = \"model6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "We now load and process the data we will need for the rest of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = load_object('objects/',model_name+'-df')\n",
    "\n",
    "scores = list(df['score'])\n",
    "num_comments_list = list(df['num_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131652"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Our Saved matricies\n",
    "PostsByWords    = load_object('matricies/',model_name+\"-PostsByWords\")\n",
    "WordsByFeatures = load_object('matricies/',model_name+\"-WordsByFeatures\")\n",
    "\n",
    "# Generate the posts by Features matrix through matrix multiplication\n",
    "PostsByFeatures = PostsByWords.dot(WordsByFeatures)\n",
    "PostsByFeatures = np.matrix(PostsByFeatures)\n",
    "len(PostsByFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('models/'+model_name+'.model')\n",
    "\n",
    "vocab_list = sorted(list(model.wv.vocab))\n",
    "\n",
    "# Initialize a word clustering to use\n",
    "num_word_clusters = 100\n",
    "kmeans =  load_object('clusters/',model_name+'-words-cluster_model-'+str(num_word_clusters))\n",
    "\n",
    "clusters = make_clustering_objects(model,kmeans,vocab_list,WordsByFeatures)\n",
    "\n",
    "clusterWords = list(map(lambda x: list(map( lambda y: y[0] ,x[\"word_list\"])), clusters))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer(vocabulary =vocab_list,analyzer=(lambda lst:list(map((lambda s: s),lst))),min_df=0)\n",
    "\n",
    "# Make Clusters By Words Matrix\n",
    "ClustersByWords = countvec.fit_transform(clusterWords)\n",
    "\n",
    "# Ensure consistency\n",
    "len(WordsByFeatures)==ClustersByWords.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take the transpose of Clusters\n",
    "WordsByCluster = ClustersByWords.transpose()\n",
    "\n",
    "# Multiply Posts by Words by Words By cluster to get Posts By cluster\n",
    "PostsByClusters = PostsByWords.dot(WordsByCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "for row in PostsByClustersNormed.tolist():\n",
    "    for col in row:\n",
    "        if (math.isnan(col)):\n",
    "            print (\"mep\")\n",
    "            print(col)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PostsByClusters=PostsByClusters.todense() *1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_min = PostsByFeatures.min(axis=1)\n",
    "row_max = PostsByFeatures.max(axis=1)\n",
    "PostsByFeaturesNormed = np.nan_to_num((PostsByFeatures-row_min)/ (row_max-row_min))\n",
    "\n",
    "row_min = PostsByClusters.min(axis=1)\n",
    "row_max = PostsByClusters.max(axis=1)\n",
    "PostsByClustersNormed =np.nan_to_num((PostsByClusters-row_min)/ (row_max-row_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PostsByClusters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PostsByClustersNormed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PostsByFeaturesNormed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Post Clusters\n",
    "\n",
    "We now will generate post clusters, and then save them in a format conducive to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_posts_clusters =10\n",
    "matricies = [PostsByFeatures,PostsByClusters,PostsByFeaturesNormed,PostsByClustersNormed]\n",
    "names     = [\"byFeatures\",\"byClusters\",\"byFeatures-Normed\",\"byClusters-Normed\"]\n",
    "mat_names = list(zip(matricies,names))\n",
    "post_dfs  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for mat,name in mat_names:\n",
    "    #initialize kmeans model\n",
    "    kmeans = KMeans(n_clusters=num_posts_clusters, random_state=42).fit(mat)\n",
    "    # Save the clusters directory\n",
    "    save_object(kmeans,'clusters/',model_name+\"-posts-\"+name+\"-\"+str(num_posts_clusters))\n",
    "    del kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = ['total_posts','score_mean','score_median','score_range','comments_mean','comments_median','comments_range']\n",
    "for mat,name in mat_names:\n",
    "    kmeans = load_object('clusters/',model_name+\"-posts-\"+name+\"-\"+str(num_posts_clusters))\n",
    "    post_clusters = make_post_clusters(kmeans,mat,scores,num_comments_list)\n",
    "    lst= mat.tolist()\n",
    "    temp_header =header+list(map(lambda x:\"element \"+str(x),range(1,mat.shape[1]+1)))\n",
    "    temp_table = list(map(lambda x: list(map(lambda y: x[1][y],header))+lst[x[0]],enumerate(post_clusters)))\n",
    "    post_dfs.append(pd.DataFrame.from_records(temp_table,columns =temp_header))\n",
    "\n",
    "    import csv\n",
    "    with open('post-analysis/'+model_name+'-'+str(num_posts_clusters)+'-'+name+'.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(temp_header)\n",
    "        [writer.writerow(r) for r in temp_table]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un normed Posts by Features Clusters\n",
    "\n",
    "We now will make un-normalized posts by features clusters, and then save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize kmeans model\n",
    "kmeans = KMeans(n_clusters=num_posts_clusters, random_state=42).fit(PostsByFeatures)\n",
    "# Save the clusters directory\n",
    "save_object(kmeans,'clusters/',model_name+\"-posts-byFeatures-cluster_model-\"+str(num_posts_clusters))\n",
    "del kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = load_object('clusters/',model_name+\"-posts-byFeatures-cluster_model-\"+str(num_posts_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_clusters = make_post_clusters(kmeans,PostsByFeatures,scores,num_comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_clusters[0]['score_mean']\n",
    "post_clusters[0]['score_median']\n",
    "post_clusters[0]['score_range']\n",
    "post_clusters[0]['total_posts']\n",
    "\n",
    "post_clusters[0]['comments_mean']\n",
    "post_clusters[0]['comments_median']\n",
    "post_clusters[0]['comments_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = ['total_posts','score_mean','score_median','score_range','comments_mean','comments_median','comments_range']\n",
    "\n",
    "temp_table = list(map(lambda x: list(map(lambda y: x[y],header)),post_clusters))\n",
    "\n",
    "posts_by_features_df = pd.DataFrame.from_records(temp_table,columns =header)\n",
    "\n",
    "import csv\n",
    "with open('post-analysis/'+model_name+'-'+str(num_posts_clusters)+'-byFeatures'+'.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    [writer.writerow(r) for r in temp_table]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un normed Posts by Clusters Clusters\n",
    "\n",
    "We now will make un-normalized Posts by Clusters clusters, and then save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize kmeans model\n",
    "kmeans = KMeans(n_clusters=num_posts_clusters, random_state=42).fit(PostsByClusters)\n",
    "# Save the clusters directory\n",
    "save_object(kmeans,'clusters/',model_name+\"-posts-byClusters-cluster_model-\"+str(num_posts_clusters))\n",
    "del kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = load_object('clusters/',model_name+\"-posts-byClusters-cluster_model-\"+str(num_posts_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = ['total_posts','score_mean','score_median','score_range','comments_mean','comments_median','comments_range']\n",
    "\n",
    "temp_table = list(map(lambda x: list(map(lambda y: x[y],header)),post_clusters))\n",
    "\n",
    "posts_by_cluster_df = pd.DataFrame.from_records(temp_table,columns =header)\n",
    "\n",
    "import csv\n",
    "with open('post-analysis/'+model_name+'-'+str(num_posts_clusters)+'-byClusters'+'.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    [writer.writerow(r) for r in temp_table]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normed Posts by Features Clusters\n",
    "\n",
    "We now will make normalized Posts by Clusters clusters, and then save them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normed Posts by Clusters Clusters\n",
    "\n",
    "We now will make normalized Posts by Clusters clusters, and then save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
