{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Models\n",
    "This notebook will walk you through building and saving the most basic \n",
    "models we used for analyzing our text data.\n",
    "\n",
    "We first import the libraries and utility files we are going to be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import useful mathematical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import useful Machine learning libraries\n",
    "import gensim\n",
    "\n",
    "# Import utility files\n",
    "from utils import read_df, remove_links, clean_sentence, save_object, load_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup directories\n",
    "\n",
    "If this is the first time doing this analysis, \n",
    "we first will set up all the directories we need\n",
    "to save and load the models we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "directories = ['objects', 'models', 'clusters', 'matricies']\n",
    "for dirname in directories:\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name Model\n",
    "\n",
    "Before begining the rest of our project, we select a name for our model.\n",
    "This name will be used to save and load the files for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"example_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse and Clean Data\n",
    "\n",
    "We first parse and clean our data. Our data is assumed to be in csv format, \n",
    "in a directory labeled 'data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the data from the csv\n",
    "df = read_df('data',extension = \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do an inspection of our data to ensure nothing went wrong\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean the text in the dataframe\n",
    "df = df.replace(np.nan, '', regex = True)\n",
    "df = df.replace(\"\\[deleted\\]\", '', regex = True)\n",
    "df[\"rawtext\"] = df[\"title\"] + \" \" + df[\"selftext\"]\n",
    "df[\"cleantext\"] = df[\"rawtext\"].apply(remove_links).apply(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that the cleaning was successful\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Analysis\n",
    "\n",
    "After parsing and cleaning the data we run the gensim phraser\n",
    "tool on our text data to join phrases like \"new york city\" \n",
    "together to form the word \"new_york_city\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a stream of tokens\n",
    "posts = df[\"cleantext\"].apply(lambda str: str.split()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a phraseDetector to join two word phrases together\n",
    "two_word_phrases = gensim.models.Phrases(posts)\n",
    "two_word_phraser = gensim.models.phrases.Phraser(two_word_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a phraseDetector to join three word phrases together\n",
    "three_word_phrases = gensim.models.Phrases(two_word_phraser[posts])\n",
    "three_word_phraser = gensim.models.phrases.Phraser(three_word_phrases)\n",
    "posts = list(three_word_phraser[two_word_phraser[posts]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update Data frame\n",
    "df[\"phrasetext\"] = df[\"cleantext\"].apply(lambda str: \" \".join(three_word_phraser[two_word_phraser[str.split()]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure posts contain same number of elements\n",
    "len(posts) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that the dataframe was updated correctly\n",
    "for i in range(len(posts)):\n",
    "    if not \" \".join(posts[i]) == list(df[\"phrasetext\"])[i]:\n",
    "        print(\"index :\" + str(i) + \" is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Saving\n",
    "\n",
    "After cleaning and parsing all of our data, we can now\n",
    "save it, so that we can analysis it later without having\n",
    "to go through lengthy computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_object(posts, 'objects/', model_name + \"-posts\")\n",
    "save_object(df, 'objects/', model_name + \"-df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Word2Vec Model\n",
    "\n",
    "After all of our data has been parsed and saved, \n",
    "we generate our Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the minimum word count to 10. This removes all words that appear less than 10 times in the data\n",
    "minimum_word_count = 10\n",
    "# Set skip gram to 1. This sets gensim to use the skip gram model instead of the Continuous Bag of Words model\n",
    "skip_gram = 1\n",
    "# Set Hidden layer size to 300.\n",
    "hidden_layer_size = 300\n",
    "# Set the window size to 5. \n",
    "window_size = 5\n",
    "# Set hierarchical softmax to 1. This sets gensim to use hierarchical softmax\n",
    "hierarchical_softmax = 1\n",
    "# Set negative sampling to 20. This is good for relatively small data sets, but becomes harder for larger datasets\n",
    "negative_sampling = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = gensim.models.Word2Vec(posts, min_count = minimum_word_count, sg = skip_gram, size = hidden_layer_size,\n",
    "                               window = window_size, hs = hierarchical_softmax, negative = negative_sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Model test\n",
    "\n",
    "After generating our model, we run some basic tests\n",
    "to ensure that it has captured some semantic information results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive = [\"kitten\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive = [\"father\", \"woman\"], negative = [\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive = [\"family\", \"obligation\"], negative = [\"love\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "After generating our model, and runing some basic tests,\n",
    "we now save it so that we can analysis it later without having\n",
    "to go through lengthy computations. We also delete and then reload\n",
    "the model, as an example of how to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('models/' + model_name + '.model')\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('models/' + model_name + '.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Matricies\n",
    "\n",
    "After generating our Word2Vec Model, we generate \n",
    "a collection of matricies that will be useful for\n",
    "analysis. This includes a Words By feature matrix,\n",
    "and a Post By Words Matrix. Note, we will use camelCase \n",
    "for matrix names, and only matrix names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the list of words used\n",
    "vocab_list = sorted(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the word vectors\n",
    "vecs = []\n",
    "for word in vocab_list:\n",
    "    vecs.append(model.wv[word].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change array format into numpy array\n",
    "WordsByFeatures = np.array(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer(vocabulary = vocab_list, analyzer = (lambda lst:list(map((lambda s:s), lst))), min_df = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make Posts By Words Matrix\n",
    "PostsByWords = countvec.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Matrix tests\n",
    "\n",
    "After generating our matricies, we run some basic tests\n",
    "to ensure that they seem resaonable later without having\n",
    "to go through lengthy computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that PostsByWords is the number of Posts by the number of words\n",
    "PostsByWords.shape[0] == len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check that the number of words is consistant for all matricies\n",
    "PostsByWords.shape[1] == len(WordsByFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Matricies\n",
    "\n",
    "After generating our matricies, we save them so we can \n",
    "analyze them later without having to go through lengthy\n",
    "computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_object(PostsByWords,'matricies/', model_name + \"-PostsByWords\")\n",
    "save_object(WordsByFeatures,'matricies/', model_name + \"-WordsByFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Word Clusters\n",
    "\n",
    "Now that we have generated and saved our matricies,\n",
    "we will proceed to generate word clusters using \n",
    "kmeans clustering, and save them for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# get the fit for different values of K\n",
    "test_points = [12] + list(range(25, 401, 25))\n",
    "fit = []\n",
    "for point in test_points:\n",
    "    kmeans = KMeans(n_clusters = point, random_state = 42).fit(WordsByFeatures)\n",
    "    save_object(kmeans, 'clusters/', model_name + \"-words-cluster_model-\" + str(point))\n",
    "    fit.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_object(fit, 'objects/', model_name + \"-words\" + \"-fit\")\n",
    "save_object(test_points, 'objects/', model_name + \"-words\" + \"-test_points\")\n",
    "del fit\n",
    "del test_points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
