import gensim, logging, os, csv, glob, re, pickle
import pandas as pd
from sklearn.cluster import KMeans
import numpy as np

""" Handle reading and parsing data"""

"""
Helper function for reading from a directory of CSVs
dirname: the name of the directory of CSVs
key: The name of the variable to extract from the CSV
returns a generator that returns the next string in the file
"""
def read(dirname,keys):
        for fname in os.listdir(dirname):
            first = True
            locs =[]
            with open(dirname+ "/"+fname,'r') as csvfile:                
                reader = csv.reader(csvfile,dialect='excel',delimiter=',',quotechar='\"')
                try:
                    for row in reader :
                        # get the location the variable to extract
                        if first:
                            for key in keys:
                                locs.append(row.index(key))
                            first = False
                        else:
                            sentence =""
                            for loc in locs:
                                sentence=sentence+" "+row[loc]
                            yield sentence             
                except:
                    print (fname + " has an error")
                csvfile.close()

"""
  Helper function for reading from a directory of CSVs into a pandas dataframe    dirname: the name of the directory of CSVs
"""
def read_df(dirname, extension = "/*.csv"):
    frame = pd.DataFrame()
    df_list =[]
    fnames = glob.glob(dirname + extension)
    for fname in fnames:
        df = pd.read_csv(fname,header=0)
        df_list.append(df)
    frame = pd.concat(df_list)
    return frame
        

# Takes a string and returns a cleaned version                
def clean_sentence(sentence):
    # remove case
    sentence = sentence.lower()
    # remove special characters
    exclude = "[•…“”\_\-,.;:\)\(\[\]0123456789/?&#%+@\\\=\*$\"!\r~\n\^]"
    temp = re.sub(exclude," ",sentence)
    return re.sub("[^a-z\ ]","",temp)
"""
    Takes a string and returns a version where all links have been
    replaced with the word link
"""
def remove_links(sentence):
    pattern="http://[^ \n\r]*"
    return re.sub(pattern," link ",sentence)


"""
Save and load utility files
Taken in part from:
https://stackoverflow.com/questions/19201290/how-to-save-a-dictionary-to-a-file
"""
def save_object(obj,dir, name ):
    with open(dir + name + '.pkl', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_object(dir,name):
    with open(dir + name + '.pkl', 'rb') as f:
        return pickle.load(f)


    """" Analyze Cluster functions"""

"""
Make Clustering Objects
Takes the kmeans object generated by sklearn, the number of clusters, and the
matrix of word vectors
and uses this to generate a list of clusters formated to contain the number
of unique words in the cluster, the total frequency of those words in the
corpus, and a list of all words, with their associate frequencies
"""
def make_clustering_objects(word2vecModel,kmeans,vocab_list,WordByFeatureMat):
    clusters =[]
    for i in range(len(kmeans.cluster_centers_)):
        clusters.append( {'unique_words':0,'total_freq':0,'word_list':[]})
    
    predictions = kmeans.predict(WordByFeatureMat)
    for i in range(len(vocab_list)):
        cluster   = predictions[i]
        word      = vocab_list[i]
        freq      = word2vecModel.wv.vocab[word].count
        clusters[cluster]['unique_words'] += 1
        clusters[cluster]['total_freq'] += freq
        clusters[cluster]['word_list'].append((word,freq))
    return clusters


"""
Make Clustering Objects
Takes the kmeans object generated by sklearn, the number of clusters, and the
matrix of word vectors
and uses this to generate a list of clusters formated to contain the number
of unique words in the cluster, the term-frequency inverse document frequency of those words in the
corpus, and a list of all words, with their associate frequencies
"""
def make_clustering_objects_tfidf(word2vecModel,kmeans,vocab_list,tfidf_list,WordByFeatureMat):
    clusters =[]
    for i in range(len(kmeans.cluster_centers_)):
        clusters.append( {'unique_words':0,'total_freq':0,'word_list':[]})
    
    predictions = kmeans.predict(WordByFeatureMat)
    for i in range(len(vocab_list)):
        cluster   = predictions[i]
        word      = vocab_list[i]
        freq      = word2vecModel.wv.vocab[word].count
        tfidf     = tfidf_list[i]
        clusters[cluster]['unique_words'] += 1
        clusters[cluster]['total_freq'] += freq
        clusters[cluster]['word_list'].append((word,tfidf))
    return clusters

    
"""
Make Clustering Objects
Takes the kmeans object generated by sklearn, the number of clusters, and the
matrix of posts vectors
and uses this to generate a list of clusters formated to contain the number
 of posts in this cluster, the mean score, the median score, and the range
"""
def make_post_clusters(kmeans,PostsByXMat,scores,num_comments_list):
    clusters =[]
    num_post_clusters = len(kmeans.cluster_centers_)
    num_features = len(kmeans.cluster_centers_[0])
    for i in range(num_post_clusters):
        clusters.append( {'total_posts':0,'post_list':[],
                          'center':list(zip(kmeans.cluster_centers_[i].tolist(),range(1,1+num_features)))})
    predictions = kmeans.predict(PostsByXMat)
    posts_vecs = PostsByXMat.tolist()
    for i in range(len(posts_vecs)):
        cluster = predictions[i]
        clusters[cluster]['total_posts'] +=1
        clusters[cluster]['post_list'].append({'score':scores[i], 'num_comments':num_comments_list[i],
                                              'vector':list(zip(posts_vecs[i],range(1,1+num_features)))})
    for cluster in clusters:
        cluster['post_list'].sort(key = (lambda x: x['score']))
        total_score=0
        total_comments=0
        min_score    = cluster['post_list'][0]['score']
        max_score    = cluster['post_list'][-1]['score']
        cluster['score_range'] = max_score-min_score
        midpoint=len(cluster['post_list'])//2
        cluster['score_median']  = cluster['post_list'][midpoint]['score']
        cluster['post_list'].sort(key = (lambda x: x['num_comments']))
        min_comments = cluster['post_list'][0]['num_comments']
        max_comments = cluster['post_list'][-1]['num_comments']
        cluster['comments_range'] = max_comments-min_comments
        for post in cluster['post_list']:
            total_score += post['score']
            total_comments += post['num_comments']
        cluster['score_mean']= total_score/cluster['total_posts']
        cluster['comments_mean'] = total_comments/cluster['total_posts']
        cluster['comments_median']  = cluster['post_list'][midpoint]['num_comments']
    return clusters


def wrapper(func, *args, **kwargs):
    def wrapped():
        return func(*args, **kwargs)
    return wrapped
