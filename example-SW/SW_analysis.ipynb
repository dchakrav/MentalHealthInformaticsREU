{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suicide Watch analysis\n",
    "This notebook will walk you through building the models we\n",
    "built after collecting our data from the Suicide Watch Subreddit\n",
    "\n",
    "We first import the libraries and utility files we are going to be using,\n",
    "and parse and clean our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import machine learning libraries\n",
    "import gensim\n",
    "import textmining\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "# Import utility files\n",
    "import dataUtils\n",
    "import clusterUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the data from the csv\n",
    "df = dataUtils.read_df('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the text in the datafram\n",
    "df =df.replace(np.nan, '', regex=True)\n",
    "df[\"rawtext\"]= df[\"title\"]+\" \"+df[\"selftext\"]\n",
    "df[\"cleantext\"]=df[\"rawtext\"].apply(dataUtils.remove_links).apply(dataUtils.cleanSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a stream of text\n",
    "posts= df[\"cleantext\"].apply(lambda str: str.split()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a phraseDetector\n",
    "two_word_phrases = gensim.models.Phrases(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "two_word_phraser = gensim.models.phrases.Phraser(two_word_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# phrase_length =3\n",
    "#posts = list(two_word_phraser[posts])\n",
    "three_word_phrases = gensim.models.Phrases(two_word_phraser[posts])\n",
    "three_word_phraser = gensim.models.phrases.Phraser(three_word_phrases)\n",
    "posts              = list(three_word_phraser[two_word_phraser[posts]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update clean text\n",
    "df[\"cleantext\"]=df[\"cleantext\"].apply(lambda str: \" \".join(three_word_phraser[two_word_phraser[str.split()]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data summary statistics\n",
    "\n",
    "Before building models, we first look at that data that we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the number of posts\n",
    "num_posts = len(posts)\n",
    "num_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the number of users (minus [deleted])\n",
    "userList= df[\"author\"].tolist()\n",
    "userDict = {}\n",
    "for user in userList:\n",
    "    if user in userDict.keys() and user != \"[deleted]\":\n",
    "        userDict[user] =1+userDict[user]\n",
    "    else:\n",
    "        userDict[user] =1\n",
    "len(list(userDict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build word2vec model\n",
    "At this step we will build the word2vec model that we will use in the rest of the analysis.\n",
    "Becuase this is a compuationally expensive process, we save the results of running our model\n",
    "as the value of model_name +\".model\" in the models directory. We can then load this model later, and do not need\n",
    "to re build it every time we want to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"model4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataUtils.save_object(posts,'objects/',model_name+\"-posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts = dataUtils.load_object('objects/',model_name+\"-posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = gensim.models.Word2Vec(posts,min_count =10,\n",
    "                               sg=1, size =300,window=5,hs=1,negative=20)\n",
    "model.save('models/'+model_name+'.model')\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 0.48737263679504395),\n",
       " ('dog', 0.4538121819496155),\n",
       " ('german_shepherd', 0.44304412603378296),\n",
       " ('baby', 0.41745078563690186),\n",
       " ('pet', 0.41535550355911255),\n",
       " ('cats', 0.41435688734054565),\n",
       " ('adopted', 0.40623027086257935),\n",
       " ('puppy', 0.4011327028274536),\n",
       " ('pup', 0.3940452039241791),\n",
       " ('kittens', 0.38637271523475647)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "model = gensim.models.Word2Vec.load('models/'+model_name+'.model')\n",
    "# Test the model: you should see cat somewhere in this list, near the top\n",
    "model.most_similar(positive=[\"kitten\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Model\n",
    "\n",
    "At this step we run some basic tests to ensure that the model has picked up on some of the semantic meanings of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"kitten\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"heartbreak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"pills\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"knife\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"heartbreak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"drugs\",\"hurt\"],negative =[\"help\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"drugs\",\"help\"],negative =[\"hurt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word usage summary\n",
    "\n",
    "At this step, after our model has looked at all the words, \n",
    "and filtered some out, we will look at the words used by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the list of words used\n",
    "vocab_list = sorted(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " '##',\n",
       " '###',\n",
       " '%',\n",
       " '%+',\n",
       " '%_certain',\n",
       " '%_certainty',\n",
       " '%_chance',\n",
       " '%_effective',\n",
       " '%_success_rate',\n",
       " '%_sure',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"''i\",\n",
       " \"''you\",\n",
       " \"'_'\",\n",
       " \"'_''\",\n",
       " \"'_lb\",\n",
       " \"'_lbs\",\n",
       " \"'_pounds\",\n",
       " \"'_tall\",\n",
       " \"'a\",\n",
       " \"'all\",\n",
       " \"'bad'\",\n",
       " \"'be\",\n",
       " \"'being\",\n",
       " \"'best_friend'\",\n",
       " \"'better'\",\n",
       " \"'cause\",\n",
       " \"'close'\",\n",
       " \"'cool'\",\n",
       " \"'d\",\n",
       " \"'depressed'\",\n",
       " \"'depression'\",\n",
       " \"'do\",\n",
       " \"'don't\",\n",
       " \"'em\",\n",
       " \"'everything\",\n",
       " \"'family'\",\n",
       " \"'fix'\",\n",
       " \"'friend'\",\n",
       " \"'friends'\",\n",
       " \"'fuck\",\n",
       " \"'get\",\n",
       " \"'get_better'\",\n",
       " \"'get_over_it'\",\n",
       " \"'go\",\n",
       " \"'god'\",\n",
       " \"'good\",\n",
       " \"'good'\",\n",
       " \"'happy'\",\n",
       " \"'have\",\n",
       " \"'help'\",\n",
       " \"'hey\",\n",
       " \"'home'\",\n",
       " \"'how\",\n",
       " \"'i\",\n",
       " \"'i'm\",\n",
       " \"'if\",\n",
       " \"'in\",\n",
       " \"'it\",\n",
       " \"'it'\",\n",
       " \"'it's\",\n",
       " \"'it_gets_better'\",\n",
       " \"'it_will\",\n",
       " \"'its\",\n",
       " \"'just\",\n",
       " \"'life\",\n",
       " \"'life'\",\n",
       " \"'living'\",\n",
       " \"'love\",\n",
       " \"'love'\",\n",
       " \"'m\",\n",
       " \"'make\",\n",
       " \"'maybe\",\n",
       " \"'me'\",\n",
       " \"'my\",\n",
       " \"'no\",\n",
       " \"'normal'\",\n",
       " \"'not\",\n",
       " \"'oh\",\n",
       " \"'okay'\",\n",
       " \"'one_more_day'\",\n",
       " \"'out\",\n",
       " \"'real\",\n",
       " \"'real'\",\n",
       " \"'right'\",\n",
       " \"'s\",\n",
       " \"'see\",\n",
       " \"'smart'\",\n",
       " \"'suicidal'\",\n",
       " \"'suicide\",\n",
       " \"'that\",\n",
       " \"'the\",\n",
       " \"'there\",\n",
       " \"'this\",\n",
       " \"'til\",\n",
       " \"'till\",\n",
       " \"'to\",\n",
       " \"'too\",\n",
       " \"'ve\",\n",
       " \"'we\",\n",
       " \"'well\",\n",
       " \"'what\",\n",
       " \"'when\",\n",
       " \"'why\",\n",
       " \"'you\",\n",
       " \"'you're\",\n",
       " '+',\n",
       " '+_hour',\n",
       " '+_hours',\n",
       " '+_miles',\n",
       " '+_miles_away',\n",
       " '+_years',\n",
       " '-',\n",
       " '--',\n",
       " '---',\n",
       " '----',\n",
       " '-----',\n",
       " '-------',\n",
       " '--------',\n",
       " '---------',\n",
       " '---_gt',\n",
       " '--_gt',\n",
       " '--and',\n",
       " '--i',\n",
       " '-_-_-talk',\n",
       " '-_calories',\n",
       " '-_hours_per',\n",
       " '-a',\n",
       " '-and',\n",
       " '-day',\n",
       " '-have',\n",
       " '-he',\n",
       " '-hour',\n",
       " '-htp',\n",
       " '-i',\n",
       " \"-i'm\",\n",
       " \"-i've\",\n",
       " '-if',\n",
       " '-im',\n",
       " '-ish',\n",
       " '-month',\n",
       " '-my',\n",
       " '-no',\n",
       " '-not',\n",
       " '-she',\n",
       " '-something',\n",
       " '-the',\n",
       " '-this',\n",
       " '-to',\n",
       " '-week',\n",
       " '-year',\n",
       " '-year-old',\n",
       " '-year-old_female',\n",
       " '-year-old_girl',\n",
       " '-year-old_male',\n",
       " '-year_old',\n",
       " '-years-old',\n",
       " '-you',\n",
       " '=',\n",
       " '=d',\n",
       " '@',\n",
       " '@gmail_com',\n",
       " '\\\\',\n",
       " '\\\\_\\\\_\\\\_\\\\',\n",
       " '`',\n",
       " 'a',\n",
       " \"a's\",\n",
       " 'a+',\n",
       " 'a-level',\n",
       " 'a-levels',\n",
       " 'a-okay',\n",
       " 'aa',\n",
       " 'aa_meetings',\n",
       " 'aaron',\n",
       " 'ab',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abandonment_issues',\n",
       " 'abandonned',\n",
       " 'abandons',\n",
       " 'abated',\n",
       " 'abby',\n",
       " 'abd',\n",
       " 'abdomen',\n",
       " 'abdominal',\n",
       " 'abducted',\n",
       " 'abhor',\n",
       " 'abhorrent',\n",
       " 'abide_by',\n",
       " 'abilify',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abit',\n",
       " 'abject',\n",
       " 'able',\n",
       " 'abnormal',\n",
       " 'abnormality',\n",
       " 'abnormally',\n",
       " 'abomination',\n",
       " 'abort',\n",
       " 'aborted',\n",
       " 'aborting',\n",
       " 'abortion',\n",
       " 'abortions',\n",
       " 'abound',\n",
       " 'about',\n",
       " 'above',\n",
       " 'above_average',\n",
       " 'above_average_intelligence',\n",
       " 'above_water',\n",
       " 'abrasive',\n",
       " 'abridged_version',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abs',\n",
       " 'absence',\n",
       " 'absences',\n",
       " 'absent',\n",
       " 'absent-minded',\n",
       " 'absolute',\n",
       " 'absolute_best',\n",
       " 'absolute_best_friend',\n",
       " 'absolute_bliss',\n",
       " 'absolute_certainty',\n",
       " 'absolute_hardest',\n",
       " 'absolute_hell',\n",
       " 'absolute_misery',\n",
       " 'absolute_nothingness',\n",
       " 'absolute_shit',\n",
       " 'absolute_trash',\n",
       " 'absolute_truth',\n",
       " 'absolute_worst',\n",
       " 'absolutely',\n",
       " 'absolutely_adore',\n",
       " 'absolutely_crushed',\n",
       " 'absolutely_despise',\n",
       " 'absolutely_devastated',\n",
       " 'absolutely_devastating',\n",
       " 'absolutely_disgusted',\n",
       " 'absolutely_gorgeous',\n",
       " 'absolutely_heartbroken',\n",
       " 'absolutely_loathe',\n",
       " 'absolutely_necessary',\n",
       " 'absolutely_refuse',\n",
       " 'absolutely_terrified',\n",
       " 'absolutely_zero',\n",
       " 'absolution',\n",
       " 'absolutley',\n",
       " 'absolutly',\n",
       " 'absolve',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorbing',\n",
       " 'abstain_from',\n",
       " 'abstinence',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'abt',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abundantly_clear',\n",
       " 'abuse',\n",
       " 'abuse_neglect',\n",
       " 'abused',\n",
       " 'abused_physically',\n",
       " 'abuser',\n",
       " 'abusers',\n",
       " 'abuses',\n",
       " 'abusing',\n",
       " 'abusing_alcohol',\n",
       " 'abusing_drugs',\n",
       " 'abusive',\n",
       " 'abusive_alcoholic',\n",
       " 'abusive_asshole',\n",
       " 'abusive_behavior',\n",
       " 'abusive_childhood',\n",
       " 'abusive_ex',\n",
       " 'abusive_ex_boyfriend',\n",
       " 'abusive_father',\n",
       " 'abusive_household',\n",
       " 'abusive_marriage',\n",
       " 'abusive_parent',\n",
       " 'abusive_relationship',\n",
       " 'abusive_relationships',\n",
       " 'abusive_towards',\n",
       " 'abut',\n",
       " 'abysmal',\n",
       " 'abyss',\n",
       " 'ac',\n",
       " 'aca',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academic_advisor',\n",
       " 'academic_career',\n",
       " 'academic_failure',\n",
       " 'academic_performance',\n",
       " 'academic_probation',\n",
       " 'academic_record',\n",
       " 'academic_success',\n",
       " 'academically',\n",
       " 'academics',\n",
       " 'academy',\n",
       " 'acc',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accelerating',\n",
       " 'accelerator',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'accept_defeat',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'acceptance_letter',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepting_new_patients',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessed',\n",
       " 'accessible',\n",
       " 'accessing',\n",
       " 'accessories',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidental_death',\n",
       " 'accidentally',\n",
       " 'accidentally_sent',\n",
       " 'accidently',\n",
       " 'accidents',\n",
       " 'accolades',\n",
       " 'accommodate',\n",
       " 'accommodating',\n",
       " 'accommodation',\n",
       " 'accommodations',\n",
       " 'accompanied',\n",
       " 'accompanied_by',\n",
       " 'accompanies',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplish',\n",
       " 'accomplish_anything',\n",
       " 'accomplished',\n",
       " 'accomplished_nothing',\n",
       " 'accomplishing',\n",
       " 'accomplishing_anything',\n",
       " 'accomplishing_something',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accountant',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accrued',\n",
       " 'acct',\n",
       " 'acctually',\n",
       " 'accumulate',\n",
       " 'accumulated',\n",
       " 'accumulating',\n",
       " 'accumulation',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accusatory',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'accustomed',\n",
       " 'accutane',\n",
       " 'ace',\n",
       " 'aced',\n",
       " 'acetaminophen',\n",
       " 'ache',\n",
       " 'ached',\n",
       " 'acheive',\n",
       " 'aches',\n",
       " 'achievable',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achieving',\n",
       " 'aching',\n",
       " 'acid',\n",
       " 'acid_reflux',\n",
       " 'acing',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledgement',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acknowledgment',\n",
       " 'acl',\n",
       " 'acne',\n",
       " 'acne_scars',\n",
       " 'acquaintance',\n",
       " 'acquaintances',\n",
       " 'acquainted',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquiring',\n",
       " 'acres',\n",
       " 'across',\n",
       " 'across_town',\n",
       " 'act',\n",
       " 'act_differently',\n",
       " 'act_itself',\n",
       " 'act_score',\n",
       " 'act_upon',\n",
       " 'act_upon_them',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'acting_strange',\n",
       " 'acting_strangely',\n",
       " 'acting_upon',\n",
       " 'acting_weird',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'activated',\n",
       " 'active',\n",
       " 'active_duty',\n",
       " 'active_social',\n",
       " 'actively',\n",
       " 'actively_avoid',\n",
       " 'actively_planning',\n",
       " 'actively_seek',\n",
       " 'actively_seeking',\n",
       " 'actively_suicidal',\n",
       " 'actively_trying',\n",
       " 'activist',\n",
       " 'activites',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actual_account',\n",
       " 'actuality',\n",
       " 'actually',\n",
       " 'actualy',\n",
       " 'acupuncture',\n",
       " 'acutally',\n",
       " 'acute',\n",
       " 'acutely',\n",
       " 'acutely_aware',\n",
       " 'acutely_suicidal',\n",
       " 'ad',\n",
       " 'ad_nauseam',\n",
       " 'adam',\n",
       " \"adam's_apple\",\n",
       " 'adamant',\n",
       " 'adamantly',\n",
       " 'adapt',\n",
       " 'adapted',\n",
       " 'adapting',\n",
       " 'add',\n",
       " 'add_insult',\n",
       " 'add_onto',\n",
       " 'added',\n",
       " 'added_stress',\n",
       " 'addendum',\n",
       " 'adderal',\n",
       " 'adderall',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addicting',\n",
       " 'addiction',\n",
       " 'addictions',\n",
       " 'addictive',\n",
       " 'addictive_personality',\n",
       " 'addicts',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adhd',\n",
       " 'adhd-pi',\n",
       " 'adhd_medication',\n",
       " 'adhd_meds',\n",
       " 'adhere',\n",
       " 'adieu',\n",
       " 'adios',\n",
       " 'adjacent',\n",
       " 'adjust',\n",
       " 'adjusted',\n",
       " 'adjusting',\n",
       " 'adjustment',\n",
       " 'adjustments',\n",
       " 'admin',\n",
       " 'administer',\n",
       " 'administered',\n",
       " 'administration',\n",
       " 'administrative',\n",
       " 'administrator',\n",
       " 'administrators',\n",
       " 'admins',\n",
       " 'admirable',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admiring',\n",
       " 'admission',\n",
       " 'admissions',\n",
       " 'admit',\n",
       " 'admit_defeat',\n",
       " 'admited',\n",
       " 'admits',\n",
       " 'admittance',\n",
       " 'admitted',\n",
       " 'admittedly',\n",
       " 'admitting',\n",
       " 'admitting_defeat',\n",
       " 'adn',\n",
       " 'adolescence',\n",
       " 'adolescent',\n",
       " 'adolescents',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adopting',\n",
       " 'adoption',\n",
       " 'adoptive_family',\n",
       " 'adoptive_parents',\n",
       " 'adorable',\n",
       " 'adoration',\n",
       " 'adore',\n",
       " 'adored',\n",
       " 'adores',\n",
       " 'adrenalin',\n",
       " 'adrenaline',\n",
       " 'adrenaline_rush',\n",
       " 'adress',\n",
       " 'adrift',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'adult_children',\n",
       " 'adulthood',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advanced_classes',\n",
       " 'advancement',\n",
       " 'advancements',\n",
       " 'advances',\n",
       " 'advancing',\n",
       " 'advantage',\n",
       " 'advantages',\n",
       " 'adventure',\n",
       " 'adventures',\n",
       " 'adventurous',\n",
       " 'adverse',\n",
       " 'adversity',\n",
       " 'advertise',\n",
       " 'advertised',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advice_regarding',\n",
       " 'advices',\n",
       " 'advil',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'adviser',\n",
       " 'advises',\n",
       " 'advising',\n",
       " 'advisor',\n",
       " 'advisors',\n",
       " 'advocate',\n",
       " 'advocating',\n",
       " 'aerospace_engineering',\n",
       " 'aesthetic',\n",
       " 'af',\n",
       " 'afaik',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affected_by',\n",
       " 'affecting',\n",
       " 'affection',\n",
       " 'affection_towards',\n",
       " 'affectionate',\n",
       " 'affectionately',\n",
       " 'affections',\n",
       " 'affects',\n",
       " 'affiliated_with',\n",
       " 'affinity',\n",
       " 'affirm',\n",
       " 'affirmation',\n",
       " 'affirmations',\n",
       " 'affirmed',\n",
       " 'affirming',\n",
       " 'afflicted',\n",
       " 'affliction',\n",
       " 'afflictions',\n",
       " 'affluent',\n",
       " 'afford',\n",
       " 'afford_food',\n",
       " 'afford_rent',\n",
       " 'affordable',\n",
       " 'afforded',\n",
       " 'affording',\n",
       " 'affraid',\n",
       " 'afghanistan',\n",
       " 'afk',\n",
       " 'afloat',\n",
       " 'aforementioned',\n",
       " 'afraid',\n",
       " 'afriad',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'african_american',\n",
       " 'after',\n",
       " 'after_graduating_high',\n",
       " 'after_graduation',\n",
       " 'afterall',\n",
       " 'afterlife',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'afternoons',\n",
       " 'afterthought',\n",
       " 'afterward',\n",
       " 'afterwards',\n",
       " 'afterwords',\n",
       " 'again',\n",
       " 'again-',\n",
       " 'against',\n",
       " 'age',\n",
       " 'age_difference',\n",
       " 'age_gap',\n",
       " 'age_range',\n",
       " 'aged',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'ages_-',\n",
       " 'ages_ago',\n",
       " 'aggravate',\n",
       " 'aggravated',\n",
       " 'aggravates',\n",
       " 'aggravating',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'aggressor',\n",
       " 'agh',\n",
       " 'aging',\n",
       " 'agitated',\n",
       " 'agitation',\n",
       " 'agnostic',\n",
       " 'ago',\n",
       " 'agonising',\n",
       " 'agonizing',\n",
       " 'agonizing_pain',\n",
       " 'agonizingly',\n",
       " 'agony',\n",
       " 'agoraphobia',\n",
       " 'agoraphobic',\n",
       " 'agree',\n",
       " 'agreeable',\n",
       " 'agreed',\n",
       " 'agreed_upon',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agreements',\n",
       " 'agrees',\n",
       " 'agressive',\n",
       " 'ah',\n",
       " 'ah_well',\n",
       " 'aha',\n",
       " 'ahe',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahh',\n",
       " 'ahve',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aided',\n",
       " 'aids',\n",
       " 'ailing',\n",
       " 'ailment',\n",
       " 'ailments',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aimed_at',\n",
       " 'aiming',\n",
       " 'aimless',\n",
       " 'aimlessly',\n",
       " \"ain't\",\n",
       " 'aint',\n",
       " 'ain’t',\n",
       " 'air',\n",
       " 'air_conditioner',\n",
       " 'air_conditioning',\n",
       " 'air_force',\n",
       " 'airforce',\n",
       " 'airing',\n",
       " 'airplane',\n",
       " 'airplanes',\n",
       " 'airport',\n",
       " 'aisle',\n",
       " 'ait',\n",
       " 'aka',\n",
       " 'akin',\n",
       " 'akward',\n",
       " 'al',\n",
       " 'alabama',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alarm_clock',\n",
       " 'alarm_went',\n",
       " 'alarmed',\n",
       " 'alarming',\n",
       " 'alarms',\n",
       " 'alas',\n",
       " 'alaska',\n",
       " 'albeit',\n",
       " 'alberta',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'alchohol',\n",
       " 'alcohol',\n",
       " 'alcohol_abuse',\n",
       " 'alcohol_addiction',\n",
       " 'alcohol_poisoning',\n",
       " 'alcoholic',\n",
       " 'alcoholic_father',\n",
       " 'alcoholic_mother',\n",
       " 'alcoholics',\n",
       " 'alcoholism',\n",
       " 'alert',\n",
       " 'alerted',\n",
       " 'alerting',\n",
       " 'aleve',\n",
       " 'alex',\n",
       " 'alexis',\n",
       " 'alexithymia',\n",
       " 'algebra',\n",
       " 'ali',\n",
       " 'alice',\n",
       " 'alien',\n",
       " 'alienate',\n",
       " 'alienated',\n",
       " 'alienates',\n",
       " 'alienating',\n",
       " 'alienation',\n",
       " 'aliens',\n",
       " 'align',\n",
       " 'aligned',\n",
       " 'alignment',\n",
       " 'alike',\n",
       " 'alimony',\n",
       " 'alison',\n",
       " 'alittle',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'all-around',\n",
       " 'all-consuming',\n",
       " 'all_encompassing',\n",
       " 'all_honesty',\n",
       " 'all_intents',\n",
       " 'all_kinds',\n",
       " 'all_nighters',\n",
       " 'all_sorts',\n",
       " 'allah',\n",
       " 'allegations',\n",
       " 'alleged',\n",
       " 'allegedly',\n",
       " 'allen',\n",
       " 'allergic',\n",
       " 'allergic_reaction',\n",
       " 'allergies',\n",
       " 'allergy',\n",
       " 'alleviate',\n",
       " 'alleviated',\n",
       " 'alleviating',\n",
       " 'alley',\n",
       " 'allies',\n",
       " 'alll',\n",
       " 'allocated',\n",
       " 'allot',\n",
       " 'allotted',\n",
       " 'allow',\n",
       " 'allowance',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allready',\n",
       " 'allright',\n",
       " 'alluded',\n",
       " 'alluding',\n",
       " 'allure',\n",
       " 'alluring',\n",
       " 'allways',\n",
       " 'ally',\n",
       " 'almighty',\n",
       " 'almost',\n",
       " 'almost_daily',\n",
       " 'almost_exclusively',\n",
       " 'almost_immediately',\n",
       " 'almost_impossible',\n",
       " 'almost_jumped',\n",
       " 'almost_jumped_off',\n",
       " 'almost_killed',\n",
       " 'almost_non-stop',\n",
       " 'almost_nonstop',\n",
       " 'almost_succeeded',\n",
       " 'almost_thirty',\n",
       " 'almost_two_decades',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'along_those_lines',\n",
       " 'alongside',\n",
       " 'aloof',\n",
       " 'alopecia',\n",
       " 'alot',\n",
       " 'aloud',\n",
       " 'alpha',\n",
       " 'alpha_male',\n",
       " 'alprazolam',\n",
       " 'already',\n",
       " 'already_dead_inside',\n",
       " 'already_established',\n",
       " 'already_written',\n",
       " 'alright',\n",
       " 'als',\n",
       " 'also',\n",
       " 'alt',\n",
       " 'alt_account',\n",
       " 'alter',\n",
       " 'altercation',\n",
       " 'altered',\n",
       " 'altering',\n",
       " 'alternate',\n",
       " 'alternate_between',\n",
       " 'alternately',\n",
       " 'alternates_between',\n",
       " 'alternating',\n",
       " 'alternating_between',\n",
       " 'alternative',\n",
       " 'alternatively',\n",
       " 'alternatives',\n",
       " 'although',\n",
       " 'altogether',\n",
       " 'altough',\n",
       " 'altruism',\n",
       " 'altruistic',\n",
       " 'alway',\n",
       " 'always',\n",
       " 'always_dreamed',\n",
       " 'always_joked',\n",
       " 'always_wondered',\n",
       " 'always_wondered_why',\n",
       " 'alyssa',\n",
       " 'alzheimer',\n",
       " \"alzheimer's\",\n",
       " 'alzheimers',\n",
       " 'am',\n",
       " 'am-',\n",
       " 'am-_am',\n",
       " 'am-_pm',\n",
       " 'am_aurora',\n",
       " 'am_aurora_im',\n",
       " 'am_eternally_grateful',\n",
       " 'am_lunar_redemption',\n",
       " 'am_tiered',\n",
       " 'ama',\n",
       " 'amanda',\n",
       " 'amassed',\n",
       " 'amateur',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazement',\n",
       " 'amazes',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazingly_well',\n",
       " 'amazon',\n",
       " 'amber',\n",
       " 'ambien',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambition',\n",
       " 'ambitions',\n",
       " 'ambitious',\n",
       " 'ambivalence',\n",
       " 'ambivalent',\n",
       " 'ambulance',\n",
       " 'ambulance_came',\n",
       " 'ambulances',\n",
       " 'amd',\n",
       " 'amend',\n",
       " 'america',\n",
       " \"america's\",\n",
       " 'american',\n",
       " 'american_dream',\n",
       " 'american_foundation',\n",
       " 'american_society',\n",
       " 'americans',\n",
       " 'americorps',\n",
       " 'amicable',\n",
       " 'amicably',\n",
       " 'amid',\n",
       " 'amidst',\n",
       " 'amirite',\n",
       " 'amiss',\n",
       " 'amitriptyline',\n",
       " 'ammo',\n",
       " 'ammonia',\n",
       " 'ammount',\n",
       " 'ammunition',\n",
       " 'amnesia',\n",
       " 'among',\n",
       " 'among_other',\n",
       " 'among_other_things',\n",
       " 'amongst',\n",
       " 'amongst_other',\n",
       " 'amongst_other_things',\n",
       " 'amoral',\n",
       " 'amount',\n",
       " 'amounted',\n",
       " 'amounting',\n",
       " 'amounts',\n",
       " 'amp',\n",
       " 'amp_amp',\n",
       " 'amp_amp_amp_amp',\n",
       " 'amp_e',\n",
       " 'amp_nbsp',\n",
       " 'amp_nbsp_amp_nbsp',\n",
       " 'amp_t',\n",
       " 'amped_up',\n",
       " 'amphetamine',\n",
       " 'amphetamines',\n",
       " 'ample',\n",
       " 'amplified',\n",
       " 'amplified_by',\n",
       " 'amplifies',\n",
       " 'amplify',\n",
       " 'amputated',\n",
       " 'amputation',\n",
       " 'amsterdam',\n",
       " 'amuse',\n",
       " 'amused',\n",
       " 'amusement',\n",
       " 'amusement_park',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'an_a+',\n",
       " 'an_a-',\n",
       " 'an_abject_failure',\n",
       " 'an_abomination',\n",
       " 'an_abortion',\n",
       " 'an_above_average',\n",
       " 'an_absolute',\n",
       " 'an_absolute_mess',\n",
       " 'an_abstract',\n",
       " 'an_absurd_amount',\n",
       " 'an_abundance',\n",
       " 'an_abuser',\n",
       " 'an_abusive_alcoholic',\n",
       " 'an_abusive_father',\n",
       " 'an_abusive_household',\n",
       " 'an_abusive_relationship',\n",
       " 'an_abysmal',\n",
       " 'an_accident',\n",
       " 'an_accidental_death',\n",
       " 'an_accomplishment',\n",
       " 'an_accountant',\n",
       " 'an_achievement',\n",
       " 'an_acquaintance',\n",
       " 'an_actor',\n",
       " 'an_actress',\n",
       " 'an_actual',\n",
       " 'an_ad',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = len(vocab_list)\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_freq = 0\n",
    "for word in vocab_list:\n",
    "    total_freq += model.wv.vocab[word].count\n",
    "total_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_list =list(map(lambda s:re.sub(\"_\",\"_\",s),vocab_list))\n",
    "countvec = CountVectorizer(vocabulary =temp_list,analyzer=(lambda lst:list(map((lambda s:re.sub(\"_\",\"_\",s)),lst))),min_df=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf    = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PostsByWords = countvec.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect a bug with creating PostsByWords\n",
    "temp = PostsByWords.sum(axis=0).tolist()[0]\n",
    "ctr =0\n",
    "for i in range(len(temp)):\n",
    "    if temp[i] < model.wv.vocab[vocab_list[i]].count:\n",
    "        print(\"<:  \"+vocab_list[i],temp[i]-model.wv.vocab[vocab_list[i]].count,temp[i],model.wv.vocab[vocab_list[i]].count)\n",
    "    elif temp[i] > model.wv.vocab[vocab_list[i]].count:\n",
    "        print(\">:  \"+vocab_list[i],temp[i]-model.wv.vocab[vocab_list[i]].count,temp[i],model.wv.vocab[vocab_list[i]].count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the magnitude of the error\n",
    "sum(temp)-sum(list(map(lambda i: model.wv.vocab[vocab_list[i]].count, range(len(vocab_list)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare PostsByWords values to correct values\n",
    "PostsByWords.sum(axis=0).tolist()[0]==list(map(lambda i: model.wv.vocab[vocab_list[i]].count, range(len(vocab_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_vocab = countvec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_arr = posts_arr.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(total_arr)-sum(list(map(lambda i: model.wv.vocab[vocab_list[i]].count, range(len(vocab_list)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "for i in range(len(posts)):\n",
    "    post = posts[i]\n",
    "    for j in range(len(post)):\n",
    "        word = post[j]\n",
    "        if word == \"amusement_park\":\n",
    "            ctr = ctr+1\n",
    "print(ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Clustering\n",
    "At this step we run and analyze the KMeans clustering algorithm \n",
    "implemented by sklearn on the word vectors we got from word2vec.\n",
    "\n",
    "The first step for this proccess is to extract the word vectors,\n",
    "and the words they correspond with from the model. We then tests \n",
    "different values of K to observe the effect of the number of centers on the fit of the model.\n",
    "After this we select a value of K to use to get the clusterings. \n",
    "We then save this result in the directory \"clustures\" with the name model_name + num_centers+\".pkl\", to save future computational time\n",
    "\n",
    "We then use the kmeans model to generate a list of dictionaries, where each dictionary corresponds to a cluster, and contains following fields:\n",
    "    'unique_words': The number of different unique words in the cluster\n",
    "    'total_freq'  : The total number of times one of the words in the cluster appeared in the corpus\n",
    "    'word_list'   : A list of words in the cluster, paired with how often they appeared in the cluster\n",
    "\n",
    "Finally we print a representation of this list to a csv, so that the clusters can be manuelly inspected.\n",
    "This representation includes the number of unique words in the cluster, the total frequency of words in the cluster, and the size_words_list most frequent words in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the word vectors\n",
    "vecs = []\n",
    "for word in vocab_list:\n",
    "    vecs.append(model.wv[word].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change array format into numpy array\n",
    "WordByFeatureMat = np.array(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the fit for different values of K\n",
    "test_points = [12]+ list(range(25,401,25))\n",
    "fit = []\n",
    "for point in test_points:\n",
    "    tempMeans = KMeans(n_clusters=point, random_state=42).fit(WordByFeatureMat)\n",
    "    fit.append(tempMeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the fit values for this model\n",
    "dataUtils.save_object(fit,'objects/',model_name+\"-fit\")\n",
    "dataUtils.save_object(test_points,'objects/',model_name+\"-testpoints\")\n",
    "del fit\n",
    "del test_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the fit and test point values\n",
    "fit         = dataUtils.load_object('objects/',model_name+\"-fit\")\n",
    "test_points = dataUtils.load_object('objects/',model_name+\"-testpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit1         = dataUtils.load_object('objects/',\"model1-fit\")\n",
    "test_points1 = dataUtils.load_object('objects/',\"model1-testpoints\")\n",
    "fit2         = dataUtils.load_object('objects/',\"model2-fit\")\n",
    "test_points2 = dataUtils.load_object('objects/',\"model2-testpoints\")\n",
    "fit3         = dataUtils.load_object('objects/',\"model3-fit\")\n",
    "test_points3 = dataUtils.load_object('objects/',\"model3-testpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph the fit for different values of K\n",
    "plt.plot(test_points1,fit1,'ro')\n",
    "plt.plot(test_points2,fit2,'bo')\n",
    "plt.plot(test_points3,fit3,'yo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the number of clusters\n",
    "num_clusters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize kmeans model\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(WordByFeatureMat)\n",
    "# Save the clusters directory\n",
    "dataUtils.save_object(kmeans,'clusters/',model_name+\"-\"+str(num_clusters))\n",
    "del kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load kmeans\n",
    "kmeans = dataUtils.load_object('clusters/',model_name+\"-\"+str(num_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = clusterUtils.makeClusteringObjects(model,kmeans,vocab_list,WordByFeatureMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determine the total words in the clusters, and the total number of unique words in the clusters\n",
    "clusters_total_words  = 0\n",
    "clusters_unique_words = 0\n",
    "for cluster in clusters:\n",
    "    clusters_total_words  += cluster['total_freq']\n",
    "    clusters_unique_words += cluster['unique_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that the total number of words in clusters matches the total\n",
    "clusters_total_words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that the number of unique words in clusters matches the total number of unique words\n",
    "clusters_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print clusters\n",
    "\n",
    "Print clusters so we can analyze them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort all the words in the words list\n",
    "for cluster in clusters:\n",
    "    cluster[\"word_list\"].sort(key=lambda x:x[1],reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size_words_list =100\n",
    "table =[]\n",
    "for i in range(len(clusters)):\n",
    "    row =[]\n",
    "    row.append(\"cluster \" + str(i+1))\n",
    "    row.append(clusters[i][\"total_freq\"])\n",
    "    row.append(clusters[i][\"unique_words\"])\n",
    "    for j in range(size_words_list):\n",
    "        try:\n",
    "            row.append(clusters[i][\"word_list\"][j])\n",
    "        except:\n",
    "            break\n",
    "    table.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('clusters-'+model_name+\"-\"+str(num_clusters)+'.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    [writer.writerow(r) for r in table]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Clusters Using MDS\n",
    "\n",
    "Produce a visualization of our clusters in a low dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the model to the clusters\n",
    "mds = MDS().fit(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words= list(map(lambda x: x[0][0],map(lambda x: x[\"word_list\"],clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the embeddings\n",
    "embedding = mds.embedding_.tolist()\n",
    "x = list(map(lambda x:x[0],embedding))\n",
    "y = list(map(lambda x:x[1],embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the Graph with top words\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(x,y,'bo')\n",
    "for i in range(len(top_words)):\n",
    "    plt.annotate(top_words[i],(x[i],y[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def helper(indicies,points):\n",
    "    temp=[]\n",
    "    for i in indicies:\n",
    "        temp.append(points[i-1])\n",
    "    return temp\n",
    "\n",
    "bullying = [59,16,47]\n",
    "crime    = [31,73]\n",
    "depressive_feelings = [1,3,15,21,29,45,81,4,30]\n",
    "depressive_symptoms = [9,13,28] \n",
    "drug_abuse =[22,41,75]\n",
    "illness  = [35,87]\n",
    "failure = [68,89,90,14,19,26,52]\n",
    "prior_suicide = [27,56,79]\n",
    "psychological =[78,10,44,66,85]\n",
    "self_harm  = [5,17]\n",
    "self_image = [69,8,96]\n",
    "death_around = [76,93]\n",
    "suicidal_ideation =[36,38,57,58,97,6]\n",
    "identified =bullying+crime+depressive_feelings+depressive_symptoms\n",
    "identified = identified +drug_abuse+illness+failure+prior_suicide+psychological\n",
    "identified = identified +self_harm+self_image+death_around+suicidal_ideation\n",
    "other = [x for x in range(1,101) if x not in identified]\n",
    "all_categories = [bullying,crime,depressive_feelings,depressive_symptoms,\n",
    "                  drug_abuse,illness,failure,prior_suicide,psychological,\n",
    "                  self_harm, self_image,death_around,suicidal_ideation,other]\n",
    "colors = [\"black\" for x in all_categories]\n",
    "\n",
    "\"\"\"\n",
    "colors = [\"#ff66ff\",\"#6666ff\",\"#000099\",\n",
    "          \"#33cccc\",\"#00cc66\",\"#336600\",\n",
    "          \"#ccff33\",\"#cc6600\",\"#ff0000\",\n",
    "          \"#cc0066\",\"#ffccff\",\"#ccffff\",\"#00ff00\",\"#00ffff\"]\n",
    "\"\"\"\n",
    "#colors[0]=\"grey\"  # Bullying\n",
    "colors[2]=\"red\"   # Depressive Feelings\n",
    "#colors[4]=\"green\" # Drug Abuse\n",
    "#colors[6]=\"blue\"  # Poor performance\n",
    "colors[3]=\"magenta\" # Depressive symptoms\n",
    "colors[8]=\"cyan\" # Psychological \n",
    "\n",
    "\n",
    "# Plot the Graph with top words\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(len(all_categories)):\n",
    "    category = all_categories[i]\n",
    "    color = colors[i]\n",
    "    plt.scatter(helper(category,x),helper(category,y),color=color,s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare for regression :TODO\n",
    "\n",
    "At this step, we will initialize the matricies we need to run a linear regression algorithm.\n",
    "We will need to create a document term matrix, and a words by cluster matrix.\n",
    "We will first use sklearn's CountVectorizer function to create the document term matrix. \n",
    "We will create the words by cluster matrix by giving each word a one hot vector, with a\n",
    "one in the cluster number, and a 0 everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordDict ={}\n",
    "for sentence in posts:\n",
    "    for word in sentence:\n",
    "        if word in wordDict.keys() and word != \"[deleted]\":\n",
    "            wordDict[word] =1+wordDict[word]\n",
    "        else:\n",
    "            wordDict[word] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"cleantext\"]=df[\"cleantext\"].apply(lambda str : ' '.join(list(filter(lambda s: wordDict[s]>=10 ,str.split()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(vocabulary =vocab_list,analyzer=(lambda lst:list(map((lambda s:re.sub(\"_\",\"_\",s)),lst))),min_df=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make Posts By Words Matrix\n",
    "PostsByWords = countvec.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusterWords = list(map(lambda x: list(map( lambda y: y[0] ,x[\"word_list\"])), clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make Clusters By Words Matrix\n",
    "ClustersByWords = countvec.fit_transform(clusterWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x29272 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 29272 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ClustersByWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29272"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr = 0\n",
    "for cluster in clusters:\n",
    "    ctr += cluster[\"unique_words\"]\n",
    "ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27362117"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr = 0\n",
    "for cluster in clusters:\n",
    "    ctr += cluster[\"total_freq\"]\n",
    "ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WordsByCluster = ClustersByWords.transpose(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<131652x29272 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 13666923 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PostsByWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<29272x100 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 29272 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordsByCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PostsByCluster = PostsByWords.dot(WordsByCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<131652x100 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4017693 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PostsByCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(PostsByCluster.sum(axis=0).tolist()[0])==sum(PostsByWords.sum(axis=0).tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ups = list(df.ups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downs = list(df.downs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = list(df.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_comments=list(df.num_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize model\n",
    "from sklearn import linear_model\n",
    "ups_model   = linear_model.Lasso(alpha=0.1)\n",
    "downs_model = linear_model.Lasso(alpha=0.1)\n",
    "score_model = linear_model.Lasso(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_comments_model = linear_model.Lasso(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train ups_model\n",
    "ups_model.fit(PostsByCluster,ups)\n",
    "dataUtils.save_object(ups_model,\"objects\",model_name+\"-ups_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train downs_model\n",
    "downs_model.fit(PostsByCluster,downs)\n",
    "dataUtils.save_object(downs_model,\"objects\",model_name+\"-downs_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train score_model\n",
    "score_model.fit(PostsByCluster,score)\n",
    "dataUtils.save_object(score_model,\"objects\",model_name+\"-score_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_comments_model.fit(PostsByCluster,num_comments)\n",
    "dataUtils.save_object(score_model,\"objects\",model_name+\"-num_comments_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.11603661,  0.        ,\n",
       "       -0.0207647 , -0.        ,  0.        , -0.        ,  0.02350768,\n",
       "       -0.        ,  0.        ,  0.07909588,  0.05333024, -0.        ,\n",
       "       -0.01213867, -0.        , -0.0168772 ,  0.        ,  0.14171743,\n",
       "        0.        ,  0.02030992,  0.        , -0.        ,  0.08143078,\n",
       "        0.        ,  0.        ,  0.        ,  0.31681984,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.01968077,  0.66322467, -0.01617729,  0.        ,\n",
       "       -0.        , -0.        , -0.00381721, -0.01874248, -0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.01993662,  0.        , -0.03059792, -0.        ,\n",
       "        0.00103457,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.05678323,\n",
       "       -0.00445982,  0.00561202,  0.01194789, -0.        ,  0.05381056,\n",
       "       -0.        ,  0.00510786,  0.        ,  0.        ,  0.00951574,\n",
       "       -0.        , -0.        , -0.04907094, -0.02254963,  0.        ,\n",
       "        0.        , -0.        ,  0.        , -0.        , -0.00963537])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ups_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.01197358, -0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.00873856,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.01326633, -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        ,  0.00209596,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        , -0.00056575])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downs_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.10988917,  0.        ,\n",
       "       -0.02037996, -0.        ,  0.        , -0.        ,  0.02792335,\n",
       "       -0.        ,  0.        ,  0.06574797,  0.0567461 , -0.        ,\n",
       "       -0.013023  , -0.        , -0.02024199,  0.        ,  0.1345511 ,\n",
       "        0.        ,  0.02095982,  0.        ,  0.        ,  0.07754381,\n",
       "        0.        ,  0.        ,  0.        ,  0.28571445,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.01509528,  0.58123988, -0.01667027,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.01789511, -0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.01962351,  0.        , -0.0253479 , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.05019542,\n",
       "       -0.00473876,  0.        ,  0.01213024, -0.        ,  0.05759005,\n",
       "       -0.        ,  0.00128403,  0.        ,  0.        ,  0.00681339,\n",
       "       -0.        , -0.        , -0.04301389, -0.02004583,  0.        ,\n",
       "        0.        , -0.        ,  0.        , -0.        , -0.00867144])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ups_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ups_coef = ups_model.coef_.tolist()\n",
    "downs_coef = downs_model.coef_.tolist()\n",
    "score_coef = downs_model.coef_.tolist()\n",
    "num_comments_coef = num_comments_model.coef_.tolist()\n",
    "for i in range(len(clusters)):\n",
    "    clusters[i][\"ups\"]  = ups_coef[i]\n",
    "    clusters[i][\"downs\"]= downs_coef[i]\n",
    "    clusters[i][\"score\"]= score_coef[i]\n",
    "    clusters[i][\"num_comments\"]= num_comments_coef[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.010220117829876252,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.0,\n",
       " 0.02895358239742311,\n",
       " 0.0,\n",
       " -0.022961673824747102,\n",
       " -0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.0,\n",
       " 0.1541935612393658,\n",
       " 0.0,\n",
       " -0.038479687381242036,\n",
       " -0.0,\n",
       " -0.03641715115302902,\n",
       " 0.15034271510666028,\n",
       " 0.05511787489154548,\n",
       " 0.0,\n",
       " 0.005070442570604117,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.00878719277008099,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.0,\n",
       " 0.20036534224642463,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0073331772765867905,\n",
       " 0.014863424061206318,\n",
       " 0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " -0.07566129011313927,\n",
       " 0.21616622629750115,\n",
       " -0.003632643936580472,\n",
       " 0.0,\n",
       " -0.028209115984079432,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.03459254263951843,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " -0.003442686835202106,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.07365192202229257,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.062020632094620436,\n",
       " -0.000604338723609844,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.014678655154487515,\n",
       " -0.03804150873533185,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.0,\n",
       " -0.006696701506456538,\n",
       " 0.0423824078352613,\n",
       " -0.0,\n",
       " 0.0,\n",
       " -0.005429937930775395,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.00645779966812168,\n",
       " -0.017920884579342065,\n",
       " -0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.0,\n",
       " -0.0,\n",
       " 0.002892362434369221]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_comments_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'downs': 0.0,\n",
       " 'num_comments': 0.0,\n",
       " 'score': 0.0,\n",
       " 'total_freq': 35760,\n",
       " 'unique_words': 161,\n",
       " 'ups': 0.0,\n",
       " 'word_list': [('cut', 7314),\n",
       "  ('cutting', 3061),\n",
       "  ('knife', 2058),\n",
       "  ('blood', 1382),\n",
       "  ('wrists', 1376),\n",
       "  ('arms', 1286),\n",
       "  ('skin', 1219),\n",
       "  ('arm', 1183),\n",
       "  ('throat', 1090),\n",
       "  ('scars', 946),\n",
       "  ('wrist', 839),\n",
       "  ('legs', 770),\n",
       "  ('slit', 697),\n",
       "  ('cuts', 692),\n",
       "  ('leg', 575),\n",
       "  ('bleed', 544),\n",
       "  ('bleeding', 527),\n",
       "  ('fingers', 388),\n",
       "  ('razor', 359),\n",
       "  ('knives', 338),\n",
       "  ('blade', 334),\n",
       "  ('slitting', 327),\n",
       "  ('scratch', 316),\n",
       "  ('wounds', 248),\n",
       "  ('bruises', 244),\n",
       "  ('bathtub', 242),\n",
       "  ('scar', 233),\n",
       "  ('stab', 224),\n",
       "  ('veins', 223),\n",
       "  ('sharp', 222),\n",
       "  ('metal', 188),\n",
       "  ('stab_myself', 181),\n",
       "  ('slice', 173),\n",
       "  ('grabbing', 170),\n",
       "  ('thighs', 164),\n",
       "  ('wound', 157),\n",
       "  ('stabbing', 148),\n",
       "  ('razors', 145),\n",
       "  ('bled', 139),\n",
       "  ('tub', 138),\n",
       "  ('vein', 137),\n",
       "  ('scratching', 132),\n",
       "  ('nails', 129),\n",
       "  ('pen', 122),\n",
       "  ('blades', 121),\n",
       "  ('razor_blade', 118),\n",
       "  ('scissors', 114),\n",
       "  ('bruised', 110),\n",
       "  ('stitches', 106),\n",
       "  ('lighter', 103),\n",
       "  ('stabbing_myself', 103),\n",
       "  ('kitchen_knife', 102),\n",
       "  ('slash', 99),\n",
       "  ('slicing', 99),\n",
       "  ('needle', 98),\n",
       "  ('scratched', 98),\n",
       "  ('biting', 88),\n",
       "  ('thigh', 86),\n",
       "  ('razor_blades', 84),\n",
       "  ('inch', 81),\n",
       "  ('scratches', 77),\n",
       "  ('pencil', 74),\n",
       "  ('carved', 72),\n",
       "  ('forearm', 67),\n",
       "  ('slashing', 64),\n",
       "  ('arteries', 61),\n",
       "  ('elbow', 59),\n",
       "  ('slashed', 59),\n",
       "  ('severed', 58),\n",
       "  ('box_cutter', 56),\n",
       "  ('sliced', 56),\n",
       "  ('bruise', 55),\n",
       "  ('palm', 53),\n",
       "  ('carve', 52),\n",
       "  ('knuckles', 48),\n",
       "  ('sleeve', 46),\n",
       "  ('ankles', 45),\n",
       "  ('shaving', 43),\n",
       "  ('pocket_knife', 42),\n",
       "  ('fingernails', 40),\n",
       "  ('forearms', 40),\n",
       "  ('jugular', 40),\n",
       "  ('wrists_open', 40),\n",
       "  ('bruising', 39),\n",
       "  ('mutilated', 39),\n",
       "  ('sharp_objects', 37),\n",
       "  ('bleeds', 36),\n",
       "  ('sharp_enough', 36),\n",
       "  ('artery', 35),\n",
       "  ('bandages', 35),\n",
       "  ('bandage', 34),\n",
       "  ('blood_everywhere', 30),\n",
       "  ('slices', 30),\n",
       "  ('blood_flow', 29),\n",
       "  ('scabs', 29),\n",
       "  ('shards', 29),\n",
       "  ('steak_knife', 29),\n",
       "  ('scalpel', 28),\n",
       "  ('scar_tissue', 28),\n",
       "  ('carotid_artery', 27),\n",
       "  ('pinching', 26),\n",
       "  ('sharp_knife', 26),\n",
       "  ('bandaged', 25),\n",
       "  ('draw_blood', 25),\n",
       "  ('shard', 25),\n",
       "  ('tendons', 25),\n",
       "  ('bracelet', 23),\n",
       "  ('sharpened', 23),\n",
       "  ('arm_open', 22),\n",
       "  ('sharpest', 22),\n",
       "  ('nails_into', 21),\n",
       "  ('vertical', 21),\n",
       "  ('staples', 20),\n",
       "  ('upper_arm', 20),\n",
       "  ('deep_cuts', 19),\n",
       "  ('razorblade', 19),\n",
       "  ('sharper', 19),\n",
       "  ('couldent', 18),\n",
       "  ('gashes', 18),\n",
       "  ('old_wounds', 18),\n",
       "  ('palms', 18),\n",
       "  ('ribbons', 18),\n",
       "  ('wrist_open', 18),\n",
       "  ('plunging', 17),\n",
       "  ('throat_open', 17),\n",
       "  ('an_artery', 16),\n",
       "  ('arms_legs', 16),\n",
       "  ('gash', 16),\n",
       "  ('needed_stitches', 16),\n",
       "  ('boiling_water', 15),\n",
       "  ('drew_blood', 15),\n",
       "  ('femoral_artery', 15),\n",
       "  ('rubber_band', 15),\n",
       "  ('warm_bath', 15),\n",
       "  ('small_cuts', 14),\n",
       "  ('upper_thigh', 14),\n",
       "  ('an_axe', 13),\n",
       "  ('horizontal', 13),\n",
       "  ('lighters', 13),\n",
       "  ('stitched', 13),\n",
       "  ('upper_arms', 13),\n",
       "  ('carotid', 12),\n",
       "  ('puncture', 12),\n",
       "  ('sharp_blade', 12),\n",
       "  ('tendon', 12),\n",
       "  ('vertically', 12),\n",
       "  ('butcher_knife', 11),\n",
       "  ('fresh_cuts', 11),\n",
       "  ('gouging', 11),\n",
       "  ('jabbing', 11),\n",
       "  ('pencil_sharpener', 11),\n",
       "  ('screwdriver', 11),\n",
       "  ('sharpest_knife', 11),\n",
       "  (\"wasn't_sharp_enough\", 11),\n",
       "  ('blood_pouring', 10),\n",
       "  ('hit_an_artery', 10),\n",
       "  ('injuring', 10),\n",
       "  ('rip_open', 10),\n",
       "  ('slashes', 10),\n",
       "  ('sliced_open', 10),\n",
       "  ('slits', 10)]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
