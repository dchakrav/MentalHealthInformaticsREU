{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suicide Watch analysis\n",
    "This notebook will walk you through building the models we\n",
    "built after collecting our data from the Suicide Watch Subreddit\n",
    "\n",
    "We first import the libraries and utility files we are going to be using,\n",
    "and parse and clean our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import machine learning libraries\n",
    "import gensim\n",
    "import textmining\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import dok_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import utility files\n",
    "import dataUtils\n",
    "import clusterUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the data from the csv\n",
    "df = dataUtils.read_df('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the text for building the model\n",
    "df =df.replace(np.nan, '', regex=True)\n",
    "df[\"rawtext\"]= df[\"title\"]+\" \"+df[\"selftext\"]\n",
    "posts= df[\"rawtext\"].apply(dataUtils.cleanSentence).apply(lambda str: str.split()).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data summary statistics\n",
    "\n",
    "Before building models, we first look at that data that we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the number of posts\n",
    "num_posts = len(posts)\n",
    "num_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the number of users (minus [deleted])\n",
    "userList= df[\"author\"].tolist()\n",
    "userDict = {}\n",
    "for user in userList:\n",
    "    if user in userDict.keys() and user != \"[deleted]\":\n",
    "        userDict[user] =1+userDict[user]\n",
    "    else:\n",
    "        userDict[user] =1\n",
    "len(list(userDict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build word2vec model\n",
    "At this step we will build the word2vec model that we will use in the rest of the analysis.\n",
    "Becuase this is a compuationally expensive process, we save the results of running our model\n",
    "as the value of model_name +\".model\" in the models directory. We can then load this model later, and do not need\n",
    "to re build it every time we want to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"model1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = gensim.models.Word2Vec(posts,min_count =10,\n",
    "                               sg=1, size =300,window=5,hs=1,negative=20)\n",
    "model.save('models/'+model_name+'.model')\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = gensim.models.Word2Vec.load('models/'+model_name+'.model')\n",
    "# Test the model: you should see cat somewhere in this list, near the top\n",
    "model.most_similar(positive=[\"kitten\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word usage summary\n",
    "\n",
    "At this step, after our model has looked at all the words, \n",
    "and filtered some out, we will look at the words used by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the list of words used\n",
    "vocab_list = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_words = len(vocab_list)\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_freq = 0\n",
    "for word in vocab_list:\n",
    "    total_freq += model.wv.vocab[word].count\n",
    "total_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Clustering\n",
    "At this step we run and analyze the KMeans clustering algorithm \n",
    "implemented by sklearn on the word vectors we got from word2vec.\n",
    "\n",
    "The first step for this proccess is to extract the word vectors,\n",
    "and the words they correspond with from the model. We then tests \n",
    "different values of K to observe the effect of the number of centers on the fit of the model.\n",
    "After this we select a value of K to use to get the clusterings. \n",
    "We then save this result in the directory \"clustures\" with the name model_name + num_centers+\".pkl\", to save future computational time\n",
    "\n",
    "We then use the kmeans model to generate a list of dictionaries, where each dictionary corresponds to a cluster, and contains following fields:\n",
    "    'unique_words': The number of different unique words in the cluster\n",
    "    'total_freq'  : The total number of times one of the words in the cluster appeared in the corpus\n",
    "    'word_list'   : A list of words in the cluster, paired with how often they appeared in the cluster\n",
    "\n",
    "Finally we print a representation of this list to a csv, so that the clusters can be manuelly inspected.\n",
    "This representation includes the number of unique words in the cluster, the total frequency of words in the cluster, and the size_words_list most frequent words in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the word vectors\n",
    "vecs = []\n",
    "for word in vocab_list:\n",
    "    vecs.append(model.wv[word].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change array format into numpy array\n",
    "WordByFeatureMat = np.array(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the fit for different values of K\n",
    "test_points = [12]+ list(range(25,401,25))\n",
    "fit = []\n",
    "for point in test_points:\n",
    "    tempMeans = KMeans(n_clusters=point, random_state=42).fit(WordByFeatureMat)\n",
    "    fit.append(tempMeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the fit values for this model\n",
    "dataUtils.save_object(fit,'objects/',model_name+\"-fit\")\n",
    "dataUtils.save_object(test_points,'objects/',model_name+\"-testpoints\")\n",
    "del fit\n",
    "del test_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the fit and test point values\n",
    "fit         = dataUtils.load_object('objects/',model_name+\"-fit\")\n",
    "test_points = dataUtils.load_object('objects/',model_name+\"-testpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph the fit for different values of K\n",
    "plt.plot(test_points,fit,'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the number of clusters\n",
    "num_clusters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize kmeans model\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(WordByFeatureMat)\n",
    "# Save the clusters directory\n",
    "dataUtils.save_object(kmeans,'clusters/',model_name+str(num_clusters))\n",
    "del kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load kmeans\n",
    "kmeans = dataUtils.load_object('clusters/',model_name+str(num_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = clusterUtils.makeClusteringObjects(model,kmeans,vocab_list,WordByFeatureMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determine the total words in the clusters, and the total number of unique words in the clusters\n",
    "clusters_total_words  = 0\n",
    "clusters_unique_words = 0\n",
    "for cluster in clusters:\n",
    "    clusters_total_words  += cluster['total_freq']\n",
    "    clusters_unique_words += cluster['unique_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that the total number of words in clusters matches the total\n",
    "clusters_total_words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that the number of unique words in clusters matches the total number of unique words\n",
    "clusters_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print clusters\n",
    "\n",
    "Print clusters so we can analyze them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort all the words in the words list\n",
    "for cluster in clusters:\n",
    "    cluster[\"word_list\"].sort(key=lambda x:x[1],reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size_words_list =10\n",
    "table =[]\n",
    "for i in range(len(clusters)):\n",
    "    row =[]\n",
    "    row.append(\"cluster \" + str(i+1))\n",
    "    row.append(clusters[i][\"total_freq\"])\n",
    "    row.append(clusters[i][\"unique_words\"])\n",
    "    for j in range(size_words_list):\n",
    "        row.append(clusters[i][\"word_list\"][j])\n",
    "    table.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('clusters.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    [writer.writerow(r) for r in table]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare for regression :TODO\n",
    "\n",
    "At this step, we will initialize the matricies we need to run a linear regression algorithm.\n",
    "We will need to create a document term matrix, and a words by cluster matrix.\n",
    "We will first use sklearn's CountVectorizer function to create the document term matrix. \n",
    "We will create the words by cluster matrix by giving each word a one hot vector, with a\n",
    "one in the cluster number, and a 0 everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = dataUtils.read_df('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df =df.replace(np.nan, '', regex=True)\n",
    "df[\"rawtext\"]= df[\"title\"]+\" \"+df[\"selftext\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordDict ={}\n",
    "for sentence in df[\"rawtext\"]:\n",
    "    for word in sentence.split():\n",
    "        if word in wordDict.keys() and word != \"[deleted]\":\n",
    "            wordDict[word] =1+wordDict[word]\n",
    "        else:\n",
    "            wordDict[word] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"cleantext\"]=df[\"rawtext\"].apply((lambda str : ' '.join(list(filter(lambda s: wordDict[s]>=10 ,str.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PostsByWords =countvec.fit_transform(df.cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PostsByWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PostsByFeatures = np.dot(PostsByWords,WordByFeatureMat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
