{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suicide Watch analysis\n",
    "This notebook will walk you through building the models we\n",
    "built after collecting our data from the Suicide Watch Subreddit\n",
    "\n",
    "We first import the libraries and utility files we are going to be using,\n",
    "and parse and clean our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from clusterUtils import make_post_clusters\n",
    "\n",
    "# Import machine learning libraries\n",
    "import gensim\n",
    "import textmining\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as LA\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "# Import utility files\n",
    "import dataUtils\n",
    "import clusterUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the data from the csv\n",
    "df = dataUtils.read_df('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import only one year of data\n",
    "import glob\n",
    "frame = pd.DataFrame()\n",
    "df_list =[]\n",
    "fnames = glob.glob('data' + \"/2016*.csv\")\n",
    "for fname in fnames:\n",
    "    df = pd.read_csv(fname,header=0)\n",
    "    df_list.append(df)\n",
    "frame = pd.concat(df_list)\n",
    "df = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean the text in the datafram\n",
    "df =df.replace(np.nan, '', regex=True)\n",
    "df[\"rawtext\"]= df[\"title\"]+\" \"+df[\"selftext\"]\n",
    "df[\"cleantext\"]=df[\"rawtext\"].apply(dataUtils.remove_links).apply(dataUtils.cleanSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a stream of text\n",
    "posts= df[\"cleantext\"].apply(lambda str: str.split()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a phraseDetector\n",
    "two_word_phrases = gensim.models.Phrases(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "two_word_phraser = gensim.models.phrases.Phraser(two_word_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# phrase_length =3\n",
    "#posts = list(two_word_phraser[posts])\n",
    "three_word_phrases = gensim.models.Phrases(two_word_phraser[posts])\n",
    "three_word_phraser = gensim.models.phrases.Phraser(three_word_phrases)\n",
    "posts              = list(three_word_phraser[two_word_phraser[posts]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update clean text\n",
    "df[\"cleantext\"]=df[\"cleantext\"].apply(lambda str: \" \".join(three_word_phraser[two_word_phraser[str.split()]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data summary statistics\n",
    "\n",
    "Before building models, we first look at that data that we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the number of posts\n",
    "num_posts = len(posts)\n",
    "num_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the number of users (minus [deleted])\n",
    "userList= df[\"author\"].tolist()\n",
    "userDict = {}\n",
    "for user in userList:\n",
    "    if user in userDict.keys() and user != \"[deleted]\":\n",
    "        userDict[user] =1+userDict[user]\n",
    "    else:\n",
    "        userDict[user] =1\n",
    "len(list(userDict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build word2vec model\n",
    "At this step we will build the word2vec model that we will use in the rest of the analysis.\n",
    "Becuase this is a compuationally expensive process, we save the results of running our model\n",
    "as the value of model_name +\".model\" in the models directory. We can then load this model later, and do not need\n",
    "to re build it every time we want to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"model6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataUtils.save_object(posts,'objects/',model_name+\"-posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts = dataUtils.load_object('objects/',model_name+\"-posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = gensim.models.Word2Vec(posts,min_count =10,\n",
    "                               sg=1, size =300,window=5,hs=1,negative=20)\n",
    "model.save('models/'+model_name+'.model')\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 0.4867851734161377),\n",
       " ('kitty', 0.4587540030479431),\n",
       " ('dog', 0.43468546867370605),\n",
       " ('baby', 0.42002180218696594),\n",
       " ('pet', 0.41659820079803467),\n",
       " ('chihuahua', 0.4150254726409912),\n",
       " ('puppy', 0.4139459729194641),\n",
       " ('stuffed_animal', 0.40899857878685),\n",
       " ('german_shepherd', 0.3876553177833557),\n",
       " ('bunny', 0.38353854417800903)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "model = gensim.models.Word2Vec.load('models/'+model_name+'.model')\n",
    "# Test the model: you should see cat somewhere in this list, near the top\n",
    "model.most_similar(positive=[\"kitten\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Model\n",
    "\n",
    "At this step we run some basic tests to ensure that the model has picked up on some of the semantic meanings of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"kitten\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"heartbreak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"pills\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"knife\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"heartbreak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"family\",\"obligation\"],negative =[\"love\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"drugs\",\"hurt\"],negative =[\"help\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"drugs\",\"help\"],negative =[\"hurt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word usage summary\n",
    "\n",
    "At this step, after our model has looked at all the words, \n",
    "and filtered some out, we will look at the words used by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the list of words used\n",
    "vocab_list = sorted(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_words = len(vocab_list)\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_freq = 0\n",
    "for word in vocab_list:\n",
    "    total_freq += model.wv.vocab[word].count\n",
    "total_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_list =list(map(lambda s:re.sub(\"_\",\"_\",s),vocab_list))\n",
    "countvec = CountVectorizer(vocabulary =temp_list,analyzer=(lambda lst:list(map((lambda s:re.sub(\"_\",\"_\",s)),lst))),min_df=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf    = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PostsByWords = countvec.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect a bug with creating PostsByWords\n",
    "temp = PostsByWords.sum(axis=0).tolist()[0]\n",
    "ctr =0\n",
    "for i in range(len(temp)):\n",
    "    if temp[i] < model.wv.vocab[vocab_list[i]].count:\n",
    "        print(\"<:  \"+vocab_list[i],temp[i]-model.wv.vocab[vocab_list[i]].count,temp[i],model.wv.vocab[vocab_list[i]].count)\n",
    "    elif temp[i] > model.wv.vocab[vocab_list[i]].count:\n",
    "        print(\">:  \"+vocab_list[i],temp[i]-model.wv.vocab[vocab_list[i]].count,temp[i],model.wv.vocab[vocab_list[i]].count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the magnitude of the error\n",
    "sum(temp)-sum(list(map(lambda i: model.wv.vocab[vocab_list[i]].count, range(len(vocab_list)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare PostsByWords values to correct values\n",
    "PostsByWords.sum(axis=0).tolist()[0]==list(map(lambda i: model.wv.vocab[vocab_list[i]].count, range(len(vocab_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(PostsByWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PostsByWords_tfidf = tfidf.fit_transform(PostsByWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_list = list(PostsByWords_tfidf.sum(axis=0).tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Clustering\n",
    "At this step we run and analyze the KMeans clustering algorithm \n",
    "implemented by sklearn on the word vectors we got from word2vec.\n",
    "\n",
    "The first step for this proccess is to extract the word vectors,\n",
    "and the words they correspond with from the model. We then tests \n",
    "different values of K to observe the effect of the number of centers on the fit of the model.\n",
    "After this we select a value of K to use to get the clusterings. \n",
    "We then save this result in the directory \"clustures\" with the name model_name + num_centers+\".pkl\", to save future computational time\n",
    "\n",
    "We then use the kmeans model to generate a list of dictionaries, where each dictionary corresponds to a cluster, and contains following fields:\n",
    "    'unique_words': The number of different unique words in the cluster\n",
    "    'total_freq'  : The total number of times one of the words in the cluster appeared in the corpus\n",
    "    'word_list'   : A list of words in the cluster, paired with how often they appeared in the cluster\n",
    "\n",
    "Finally we print a representation of this list to a csv, so that the clusters can be manuelly inspected.\n",
    "This representation includes the number of unique words in the cluster, the total frequency of words in the cluster, and the size_words_list most frequent words in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the word vectors\n",
    "vecs = []\n",
    "for word in vocab_list:\n",
    "    vecs.append(model.wv[word].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change array format into numpy array\n",
    "WordByFeatureMat = np.array(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the fit for different values of K\n",
    "test_points = [12]+ list(range(25,401,25))\n",
    "fit = []\n",
    "for point in test_points:\n",
    "    tempMeans = KMeans(n_clusters=point, random_state=42).fit(WordByFeatureMat)\n",
    "    fit.append(tempMeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the fit values for this model\n",
    "dataUtils.save_object(fit,'objects/',model_name+\"-fit\")\n",
    "dataUtils.save_object(test_points,'objects/',model_name+\"-testpoints\")\n",
    "del fit\n",
    "del test_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the fit and test point values\n",
    "fit         = dataUtils.load_object('objects/',model_name+\"-fit\")\n",
    "test_points = dataUtils.load_object('objects/',model_name+\"-testpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit1         = dataUtils.load_object('objects/',\"model1-fit\")\n",
    "test_points1 = dataUtils.load_object('objects/',\"model1-testpoints\")\n",
    "fit2         = dataUtils.load_object('objects/',\"model2-fit\")\n",
    "test_points2 = dataUtils.load_object('objects/',\"model2-testpoints\")\n",
    "fit3         = dataUtils.load_object('objects/',\"model3-fit\")\n",
    "test_points3 = dataUtils.load_object('objects/',\"model3-testpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph the fit for different values of K\n",
    "plt.plot(test_points1,fit1,'ro')\n",
    "plt.plot(test_points2,fit2,'bo')\n",
    "plt.plot(test_points3,fit3,'yo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the number of clusters\n",
    "num_clusters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize kmeans model\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(WordByFeatureMat)\n",
    "# Save the clusters directory\n",
    "dataUtils.save_object(kmeans,'clusters/',model_name+\"-\"+str(num_clusters))\n",
    "del kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load kmeans\n",
    "kmeans = dataUtils.load_object('clusters/',model_name+\"-\"+str(num_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = clusterUtils.make_clustering_objects_tfidf(model,kmeans,vocab_list,tfidf_list,WordByFeatureMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataUtils.save_object(clusters,'clusters/',model_name+\"-clusters_dict-\"+str(num_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determine the total words in the clusters, and the total number of unique words in the clusters\n",
    "clusters_total_words  = 0\n",
    "clusters_unique_words = 0\n",
    "for cluster in clusters:\n",
    "    clusters_total_words  += cluster['total_freq']\n",
    "    clusters_unique_words += cluster['unique_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that the total number of words in clusters matches the total\n",
    "clusters_total_words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that the number of unique words in clusters matches the total number of unique words\n",
    "clusters_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print clusters\n",
    "\n",
    "Print clusters so we can analyze them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort all the words in the words list\n",
    "for cluster in clusters:\n",
    "    cluster[\"word_list\"].sort(key=lambda x:x[1],reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size_words_list =100\n",
    "table =[]\n",
    "for i in range(len(clusters)):\n",
    "    row =[]\n",
    "    row.append(\"cluster \" + str(i+1))\n",
    "    row.append(clusters[i][\"total_freq\"])\n",
    "    row.append(clusters[i][\"unique_words\"])\n",
    "    for j in range(size_words_list):\n",
    "        try:\n",
    "            row.append(clusters[i][\"word_list\"][j])\n",
    "        except:\n",
    "            break\n",
    "    table.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('tfidf-clusters-'+model_name+\"-\"+str(num_clusters)+'.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    [writer.writerow(r) for r in table]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Clusters Using MDS\n",
    "\n",
    "Produce a visualization of our clusters in a low dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the model to the clusters\n",
    "mds = MDS().fit(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words= list(map(lambda x: x[0][0],map(lambda x: x[\"word_list\"],clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the embeddings\n",
    "embedding = mds.embedding_.tolist()\n",
    "x = list(map(lambda x:x[0],embedding))\n",
    "y = list(map(lambda x:x[1],embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the Graph with top words\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(x,y,'bo')\n",
    "for i in range(len(top_words)):\n",
    "    plt.annotate(top_words[i],(x[i],y[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def helper(indicies,points):\n",
    "    temp=[]\n",
    "    for i in indicies:\n",
    "        temp.append(points[i-1])\n",
    "    return temp\n",
    "\n",
    "bullying = [59,16,47]\n",
    "crime    = [31,73]\n",
    "depressive_feelings = [1,3,15,21,29,45,81,4,30]\n",
    "depressive_symptoms = [9,13,28] \n",
    "drug_abuse =[22,41,75]\n",
    "illness  = [35,87]\n",
    "failure = [68,89,90,14,19,26,52]\n",
    "prior_suicide = [27,56,79]\n",
    "psychological =[78,10,44,66,85]\n",
    "self_harm  = [5,17]\n",
    "self_image = [69,8,96]\n",
    "death_around = [76,93]\n",
    "suicidal_ideation =[36,38,57,58,97,6]\n",
    "identified =bullying+crime+depressive_feelings+depressive_symptoms\n",
    "identified = identified +drug_abuse+illness+failure+prior_suicide+psychological\n",
    "identified = identified +self_harm+self_image+death_around+suicidal_ideation\n",
    "other = [x for x in range(1,101) if x not in identified]\n",
    "all_categories = [bullying,crime,depressive_feelings,depressive_symptoms,\n",
    "                  drug_abuse,illness,failure,prior_suicide,psychological,\n",
    "                  self_harm, self_image,death_around,suicidal_ideation,other]\n",
    "colors = [\"black\" for x in all_categories]\n",
    "\n",
    "\"\"\"\n",
    "colors = [\"#ff66ff\",\"#6666ff\",\"#000099\",\n",
    "          \"#33cccc\",\"#00cc66\",\"#336600\",\n",
    "          \"#ccff33\",\"#cc6600\",\"#ff0000\",\n",
    "          \"#cc0066\",\"#ffccff\",\"#ccffff\",\"#00ff00\",\"#00ffff\"]\n",
    "\"\"\"\n",
    "#colors[0]=\"grey\"  # Bullying\n",
    "colors[2]=\"red\"   # Depressive Feelings\n",
    "#colors[4]=\"green\" # Drug Abuse\n",
    "#colors[6]=\"blue\"  # Poor performance\n",
    "colors[3]=\"magenta\" # Depressive symptoms\n",
    "colors[8]=\"cyan\" # Psychological \n",
    "\n",
    "\n",
    "# Plot the Graph with top words\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(len(all_categories)):\n",
    "    category = all_categories[i]\n",
    "    color = colors[i]\n",
    "    plt.scatter(helper(category,x),helper(category,y),color=color,s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare for regression :TODO\n",
    "\n",
    "At this step, we will initialize the matricies we need to run a linear regression algorithm.\n",
    "We will need to create a document term matrix, and a words by cluster matrix.\n",
    "We will first use sklearn's CountVectorizer function to create the document term matrix. \n",
    "We will create the words by cluster matrix by giving each word a one hot vector, with a\n",
    "one in the cluster number, and a 0 everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(vocabulary =vocab_list,analyzer=(lambda lst:list(map((lambda s:re.sub(\"_\",\"_\",s)),lst))),min_df=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make Posts By Words Matrix\n",
    "PostsByWords = countvec.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusterWords = list(map(lambda x: list(map( lambda y: y[0] ,x[\"word_list\"])), clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make Clusters By Words Matrix\n",
    "ClustersByWords = countvec.fit_transform(clusterWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x28663 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 28663 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ClustersByWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28663"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the number of elements in ClustersBy words is equal to the total number of words\n",
    "ctr = 0\n",
    "for cluster in clusters:\n",
    "    ctr += cluster[\"unique_words\"]\n",
    "ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take the transpose of Clusters\n",
    "WordsByCluster = ClustersByWords.transpose(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multiply Posts by Words by Words By cluster to get Posts By cluster\n",
    "PostsByCluster = PostsByWords.dot(WordsByCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "df[\"date\"]=df[\"created_utc\"].apply(datetime.fromtimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"year\"]= df[\"date\"].apply(lambda x: x.year-2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"month\"]= df[\"date\"].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"day\"]= df[\"date\"].apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"hour\"]= df[\"date\"].apply(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years = df[\"year\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "months = df[\"month\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = np.zeros((len(months), 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp[np.arange(len(months)), list(map(lambda x: x//4, months))] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "months = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years = df[\"year\"].values\n",
    "years = np.matrix(list(map(lambda x: [x],np.array(years))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "months = df[\"month\"].values\n",
    "temp = np.zeros((len(months), 4))\n",
    "temp[np.arange(len(months)), list(map(lambda x: (x-1)//3 , months))] = 1\n",
    "months = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hours = df[\"hour\"].values\n",
    "temp = np.zeros((len(hours), 6))\n",
    "temp[np.arange(len(hours)), list(map(lambda x: x//4, hours))] = 1\n",
    "hours = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time = np.concatenate((years,months,hours),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_time =np.array(time).tolist()\n",
    "temp = [None]*len(temp_time[0])\n",
    "for i in range(len(temp_time[0])):\n",
    "    temp[i] =0\n",
    "for t in temp_time:\n",
    "    for i in range(len(t)):\n",
    "        temp[i]+=t[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PostsByClusterList =PostsByCluster.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(PostsByClusterList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove high comments posts\n",
    "lst = list(df.num_comments)\n",
    "indicies =[]\n",
    "for i in range(len(lst)):\n",
    "    if lst[i] <=10:\n",
    "        indicies.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PostsByClusterList = [x for ind, x in enumerate(PostsByClusterList) if ind in indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PostsByCluster_clean = sparse.csr_matrix(PostsByClusterList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PostsByCluster_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PostsByCluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(PostsByCluster.sum(axis=0).tolist()[0])==sum(PostsByWords.sum(axis=0).tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run regression\n",
    "\n",
    "At this stage we run a regression on the normalized PostsByCluster matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize regression fields\n",
    "regression_fields = [\"ups\",\"downs\",\"score\",\"num_comments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize regression data\n",
    "regression_data=[None]*len(regression_fields)\n",
    "for i in range(len(regression_fields)):\n",
    "    regression_data[i]= (list(np.log(df[regression_fields[i]].apply(lambda x: x if x>0 else 0.1 ))))\n",
    "#    regression_data[i]= list(df[regression_fields[i]])\n",
    "#    regression_data[i]= [x for ind, x in enumerate(regression_data[i]) if ind in indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize model\n",
    "import statsmodels.api as sm\n",
    "regression_models =[None]*len(regression_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modify PostsByCluster to have a bias colum\n",
    "#X = PostsByCluster_clean.toarray()\n",
    "#X = np.array(time)\n",
    "X = np.array(np.concatenate((PostsByCluster.toarray(),time),axis=1))\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a regression\n",
    "for i in range(len(regression_fields)):\n",
    "    model = sm.OLS(regression_data[i], X)\n",
    "    regression_models[i] = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize regression coeficients\n",
    "regression_coefs = [None]*len(regression_fields)\n",
    "for i in range(len(regression_fields)):\n",
    "    regression_coefs[i]= regression_models[i].params.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(regression_fields)):\n",
    "    field =regression_fields[i]\n",
    "    for j in range(len(clusters)):\n",
    "        clusters[j][field]  = regression_coefs[i][j+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regression_coef_locs=[None]*len(regression_fields)\n",
    "\n",
    "for i in range(len(regression_coef_locs)):\n",
    "    field =regression_fields[i]\n",
    "    regression_coef_locs[i]=[]\n",
    "    for j in range(len(clusters)):   \n",
    "        if clusters[j][field] != 0.0:\n",
    "            regression_coef_locs[i].append((clusters[j][field],j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort all the words in the words list\n",
    "for cluster in clusters:\n",
    "    cluster[\"word_list\"].sort(key=lambda x:x[1],reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size_words_list =100\n",
    "regression_tables= [None]*len(regression_fields)\n",
    "for i in range(len(regression_coef_locs)):\n",
    "    lst = sorted(regression_coef_locs[i],reverse=True)\n",
    "    regression_tables[i]=[]\n",
    "    for beta,k in lst:\n",
    "        row =[]\n",
    "        row.append(regression_fields[i]+\" \" + str(k+1))\n",
    "        row.append(beta)\n",
    "        for j in range(size_words_list):\n",
    "            try:\n",
    "                row.append(clusters[k][\"word_list\"][j])\n",
    "            except:\n",
    "                break\n",
    "        regression_tables[i].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "for i in range(len(regression_fields)):\n",
    "    with open('regression-'+regression_fields[i]+'-'+model_name+'.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        [writer.writerow(r) for r in regression_tables[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# log a summary of each regression\n",
    "#regression_settings= \"log-2016+under10-clusters-regularized\"\n",
    "regression_settings= \"log-clusters+time\"\n",
    "for i in range(len(regression_fields)):\n",
    "    f = open(\"regression/\"+model_name+\"-\"+regression_settings+\"-\"+regression_fields[i]+\".txt\",\"w\")\n",
    "    f.write(str(regression_models[i].summary()))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values_array= np.array(list(zip(df['ups'].tolist(),df['downs'].tolist(),df['score'].tolist(),df['num_comments'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131652"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_table= np.concatenate((PostsByCluster.toarray(),values_array),axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = list(map(lambda x: \"cluster \"+ str(x),range(1,1+len(PostsByCluster.toarray()[0]))))+regression_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('regression_posts.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    [writer.writerow(r) for r in temp_table]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Posts\n",
    "\n",
    "In this section, we will cluster posts, similar to how we clustered word vectors above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the fit for different values of K\n",
    "test_points = [12]+ list(range(25,401,25))\n",
    "fit = []\n",
    "for point in test_points:\n",
    "    tempMeans = KMeans(n_clusters=point, random_state=42).fit(WordByFeatureMat)\n",
    "    fit.append(tempMeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the fit values for this model\n",
    "dataUtils.save_object(fit,'objects/',model_name+\"-posts_by_cluster\"+\"-fit\")\n",
    "dataUtils.save_object(test_points,'objects/',model_name+\"-posts_by_cluster\"+\"-testpoints\")\n",
    "del fit\n",
    "del test_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the fit and test point values\n",
    "fit         = dataUtils.load_object('objects/',model_name+\"-posts_by_cluster\"+\"-fit\")\n",
    "test_points = dataUtils.load_object('objects/',model_name+\"-posts_by_cluster\"+\"-testpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(test_points,fit,'yo')\n",
    "plt.axis([0, 400, 0, 260000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_posts_clusters =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-64d2ebed7a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#initialize kmeans model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_posts_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPostsByCluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Save the clusters directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'clusters/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"posts_by_cluster\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_posts_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 x_squared_norms=x_squared_norms, random_state=random_state)\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             centers = _k_means._centers_sparse(X, labels, n_clusters,\n\u001b[0;32m--> 496\u001b[0;31m                                                distances)\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_k_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_centers_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#initialize kmeans model\n",
    "kmeans = KMeans(n_clusters=num_posts_clusters, random_state=42).fit(PostsByCluster)\n",
    "# Save the clusters directory\n",
    "dataUtils.save_object(kmeans,'clusters/',model_name+\"-\"+\"posts_by_cluster\"+\"-\"+str(num_posts_clusters))\n",
    "del kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load kmeans\n",
    "kmeans = dataUtils.load_object('clusters/',model_name+\"-\"+\"posts_by_cluster\"+\"-\"+str(num_posts_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(clusterUtils)\n",
    "\n",
    "scores            = df['score'].tolist()\n",
    "num_comments_list = df['num_comments'].tolist()\n",
    "clusters = clusterUtils.make_post_clusters(kmeans,PostsByCluster,scores,num_comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cluster in clusters:\n",
    "    cluster['center'].sort(key=lambda x:x[0],reverse = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for cluster in clusters:\n",
    "    print(cluster['score_median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Record the distribution of scores\n",
    "dictArr =[{},{},{},{}]\n",
    "for i in range(len(regression_data)):\n",
    "    data = regression_data[i]\n",
    "    dictionary = dictArr[i]\n",
    "    for val in data:\n",
    "        if val in dictionary.keys():\n",
    "            dictionary[val] =1+dictionary[val]\n",
    "        else:\n",
    "            dictionary[val] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total = [None]*len(regression_fields)\n",
    "new   = [None]*len(regression_fields)\n",
    "for i in range(len(regression_fields)):\n",
    "    total[i]=0\n",
    "    new[i]=0\n",
    "    for k in dictArr[i].keys():\n",
    "        total[i] += dictArr[i][k]\n",
    "        if k<=10:\n",
    "            new[i] += dictArr[i][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check correlations between clusters and scores\n",
    "for i in range(len(clusters)):\n",
    "    df[\"cluster \"+str(i)]= PostsByCluster[:,i].toarray().tolist()\n",
    "    df[\"cluster \"+str(i)]= df[\"cluster \"+str(i)].apply(lambda x : x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(clusters)):\n",
    "    print(np.log(df['score']+1).corr(df[\"cluster \"+str(i)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
