{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Analysis\n",
    "\n",
    "This notebook focuses on analysing the word2vec model we are using. This will mostly involve testing the functions given by gensim.\n",
    "\n",
    "We first import the libaries we will need throughout the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import graphing utilities\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import useful mathematical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import useful Machine learning libraries\n",
    "import gensim\n",
    "\n",
    "# Import utility files\n",
    "from utils import save_object,load_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model name\n",
    "\n",
    "Before begining the rest of this project, we select a name for our model. This name will be used to save and load the files for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"PTSD_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine word similarities\n",
    "\n",
    "We first examine word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('models/'+model_name+'.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('neglect', 0.686439037322998),\n",
       " ('molestation', 0.6569526195526123),\n",
       " ('emotional_abuse', 0.6295863389968872),\n",
       " ('sexual_abuse', 0.6221255660057068),\n",
       " ('physical_abuse', 0.6206825971603394),\n",
       " ('child_abuse', 0.5941599011421204),\n",
       " ('domestic_abuse', 0.5852190852165222),\n",
       " ('psychological_abuse', 0.5817166566848755),\n",
       " ('domestic_violence', 0.5812309980392456),\n",
       " ('incest', 0.5779865980148315)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"abuse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tours', 0.6432517766952515),\n",
       " ('active_duty', 0.6208668351173401),\n",
       " ('army', 0.6053268909454346),\n",
       " ('vietnam', 0.5866836309432983),\n",
       " ('served', 0.5829823017120361),\n",
       " ('u_s', 0.5777806043624878),\n",
       " ('personnel', 0.5742989778518677),\n",
       " ('civilian', 0.5705838203430176),\n",
       " ('honorably', 0.5666223764419556),\n",
       " ('serving', 0.5630596280097961)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"military\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('medications', 0.674418568611145),\n",
       " ('meds', 0.6736170053482056),\n",
       " ('prozac', 0.6497324705123901),\n",
       " ('anti_depressants', 0.6424094438552856),\n",
       " ('celexa', 0.6368412375450134),\n",
       " ('wellbutrin', 0.6212787628173828),\n",
       " ('melatonin', 0.6191287636756897),\n",
       " ('valium', 0.6103256940841675),\n",
       " ('sertraline', 0.6098331212997437),\n",
       " ('ssris', 0.6066856384277344)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"medication\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('demands', 0.5837190747261047),\n",
       " ('bystander', 0.5464884042739868),\n",
       " ('rights', 0.5452970266342163),\n",
       " ('contrary', 0.5396021604537964),\n",
       " ('molestation', 0.5237962007522583),\n",
       " ('survivor', 0.5154806971549988),\n",
       " ('engagement', 0.5143830180168152),\n",
       " ('cruelty', 0.5124729871749878),\n",
       " ('authority', 0.5085850358009338),\n",
       " ('incest', 0.5041314363479614)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"victim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c_ptsd', 0.6747949123382568),\n",
       " ('pstd', 0.6720682382583618),\n",
       " ('cptsd', 0.6395062208175659),\n",
       " ('hello_forum', 0.6280524730682373),\n",
       " ('ptsd_stemming_from', 0.6165199875831604),\n",
       " ('dysthymia', 0.6154494285583496),\n",
       " ('complex_ptsd', 0.6153533458709717),\n",
       " ('mdd', 0.6142030358314514),\n",
       " ('gad', 0.6120656132698059),\n",
       " ('combat_related_ptsd', 0.6053071022033691)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"ptsd\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine word relationships\n",
    "\n",
    "We now examine information contained in word vectors relative locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('his_wife', 0.5327454805374146),\n",
       " ('killed_himself', 0.532505214214325),\n",
       " ('killing_himself', 0.5307286977767944),\n",
       " ('my', 0.5239009857177734),\n",
       " ('year_old_son', 0.5226487517356873),\n",
       " ('code', 0.5033747553825378),\n",
       " ('laughs', 0.5032817721366882),\n",
       " ('shotgun', 0.5007234811782837),\n",
       " ('then_proceeded', 0.49907752871513367),\n",
       " ('affair', 0.497707337141037)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"his\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('self_harm', 0.5812534093856812),\n",
       " ('prevention', 0.5250056982040405),\n",
       " ('bodily', 0.518842875957489),\n",
       " ('x_post', 0.5078316926956177),\n",
       " ('ptsd_resulting_from', 0.5067867636680603),\n",
       " ('tactics', 0.4994078278541565),\n",
       " ('self_harming', 0.4993220567703247),\n",
       " ('dx', 0.4893164038658142),\n",
       " ('destruction', 0.4887635111808777),\n",
       " ('medically', 0.48848235607147217)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"suicide\",\"self\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('police_officers', 0.47538650035858154),\n",
       " ('extended_family', 0.47335386276245117),\n",
       " ('relatives', 0.4723043441772461),\n",
       " ('aunts', 0.465665340423584),\n",
       " ('reputation', 0.46317481994628906),\n",
       " ('immediate_family', 0.46269842982292175),\n",
       " ('exs', 0.4467809498310089),\n",
       " ('cut_ties_with', 0.44380924105644226),\n",
       " ('norm', 0.4427856504917145),\n",
       " ('half_brother', 0.4408080577850342)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"family\",\"obligation\"],negative = [\"love\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sister', 0.47389113903045654),\n",
       " ('boyfriend', 0.46766215562820435),\n",
       " ('current_boyfriend', 0.46211689710617065),\n",
       " ('little_brother', 0.45899638533592224),\n",
       " ('husband', 0.45616835355758667),\n",
       " ('aunt', 0.4549723267555237),\n",
       " ('stepfather', 0.4539860486984253),\n",
       " ('niece', 0.4486340880393982),\n",
       " ('mum', 0.4391021132469177),\n",
       " ('bf', 0.43180689215660095)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"brother\",\"girl\"],negative = [\"boy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stepfather', 0.5219822525978088),\n",
       " ('mother', 0.5164777040481567),\n",
       " ('molested_by', 0.5076919794082642),\n",
       " ('ex_husband', 0.4981922507286072),\n",
       " ('schizophrenic', 0.4953431189060211),\n",
       " ('mentally_abusive', 0.49383193254470825),\n",
       " ('raised_by', 0.4865146279335022),\n",
       " ('half_brother', 0.48134884238243103),\n",
       " ('bio', 0.47938424348831177),\n",
       " ('physically_abusive', 0.47608622908592224)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"father\",\"woman\"],negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shelter', 0.49598196148872375),\n",
       " ('vodka', 0.48595672845840454),\n",
       " ('dressed', 0.4799622595310211),\n",
       " ('birds', 0.4784354567527771),\n",
       " ('eight', 0.47741347551345825),\n",
       " ('meal', 0.4755449891090393),\n",
       " ('president', 0.4740760028362274),\n",
       " ('stores', 0.47386157512664795),\n",
       " ('star', 0.4701704680919647),\n",
       " ('mortar', 0.46942639350891113)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"kitten\",\"dog\"],negative=[\"cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trauma_survivor', 0.6720503568649292),\n",
       " ('sexual_trauma', 0.6434582471847534),\n",
       " ('military_service', 0.6214268207550049),\n",
       " ('brain_damage', 0.6030505895614624),\n",
       " ('rights', 0.5976473689079285),\n",
       " ('resistant', 0.5975258350372314),\n",
       " ('long_term_abuse', 0.596721351146698),\n",
       " ('multiple_traumas', 0.5955366492271423),\n",
       " ('traumatic_brain_injury', 0.5949004292488098),\n",
       " ('military_related', 0.5943823456764221)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"veteran\",\"trauma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('england', 0.4719510078430176),\n",
       " ('half_brother', 0.46942847967147827),\n",
       " ('ticket', 0.4686640799045563),\n",
       " ('another_state', 0.4676247835159302),\n",
       " ('caretaker', 0.46154946088790894),\n",
       " ('sons', 0.4584343433380127),\n",
       " ('laws', 0.453119695186615),\n",
       " ('year_old_son', 0.44864320755004883),\n",
       " ('mother', 0.445003867149353),\n",
       " ('raised_by', 0.44426387548446655)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"law\",\"family\"],negative=[\"love\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
